# Uncertainty Module - å®ç°ç»†èŠ‚

> **å®ç°ç‰ˆæœ¬:** 1.0
> **çŠ¶æ€:** è§„åˆ’ä¸­
> **è§„æ ¼æ–‡æ¡£:** [uncertainty.md](../specs/uncertainty.md)
> **æœ€åæ›´æ–°:** 2025-10-22

---

## ğŸ“‚ æ–‡ä»¶ç»“æ„

```
src/modules/
â””â”€â”€ uncertainty/
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ base.py              # åŸºç±»å®šä¹‰
    â”œâ”€â”€ mc_dropout.py        # MC Dropout å®ç°
    â”œâ”€â”€ ensemble.py          # Deep Ensembles å®ç°
    â”œâ”€â”€ bayesian.py          # è´å¶æ–¯NNå®ç°
    â”œâ”€â”€ calibration.py       # æ ¡å‡†æ¨¡å—
    â””â”€â”€ metrics.py           # è¯„ä¼°æŒ‡æ ‡
```

---

## ğŸ”¨ å®ç°æ­¥éª¤

### Phase 1: åŸºç¡€æ¡†æ¶ (Week 1)

#### 1.1 åŸºç±»å®ç°

**æ–‡ä»¶:** `src/modules/uncertainty/base.py`

```python
from abc import ABC, abstractmethod
from typing import Dict, Optional
import torch
import torch.nn as nn


class UncertaintyEstimator(ABC, nn.Module):
    """
    ä¸ç¡®å®šæ€§ä¼°è®¡å™¨æŠ½è±¡åŸºç±»
    """

    def __init__(self, config: Dict):
        super().__init__()
        self.config = config
        self.calibrator = None

    @abstractmethod
    def estimate(
        self,
        x: torch.Tensor,
        model: nn.Module,
        return_samples: bool = False
    ) -> Dict[str, torch.Tensor]:
        """
        ä¼°è®¡ä¸ç¡®å®šæ€§

        Args:
            x: è¾“å…¥ç‰¹å¾ [B, D]
            model: é¢„æµ‹æ¨¡å‹
            return_samples: æ˜¯å¦è¿”å›æ‰€æœ‰é‡‡æ ·

        Returns:
            {
                'mean': å¹³å‡é¢„æµ‹ [B],
                'epistemic': è®¤çŸ¥ä¸ç¡®å®šæ€§ [B],
                'aleatoric': å¶ç„¶ä¸ç¡®å®šæ€§ [B],
                'samples': é‡‡æ ·ç»“æœ [S, B] (optional)
            }
        """
        pass

    def calibrate(
        self,
        logits: torch.Tensor,
        labels: torch.Tensor,
        method: str = "temperature_scaling"
    ):
        """
        æ ¡å‡†æ¨¡å‹

        Args:
            logits: æ¨¡å‹è¾“å‡º [N]
            labels: çœŸå®æ ‡ç­¾ [N]
            method: æ ¡å‡†æ–¹æ³•
        """
        from .calibration import get_calibrator
        self.calibrator = get_calibrator(method)
        self.calibrator.fit(logits, labels)

    def forward(
        self,
        x: torch.Tensor,
        model: nn.Module
    ) -> Dict[str, torch.Tensor]:
        """
        å‰å‘ä¼ æ’­ï¼ˆåŒ…å«æ ¡å‡†ï¼‰
        """
        results = self.estimate(x, model)

        # åº”ç”¨æ ¡å‡†
        if self.calibrator is not None:
            results['mean'] = self.calibrator(results['mean'])

        return results
```

#### 1.2 MC Dropout å®ç°

**æ–‡ä»¶:** `src/modules/uncertainty/mc_dropout.py`

```python
import torch
import torch.nn as nn
from typing import Dict
from .base import UncertaintyEstimator


class MCDropoutEstimator(UncertaintyEstimator):
    """
    Monte Carlo Dropout ä¸ç¡®å®šæ€§ä¼°è®¡

    ä½¿ç”¨æ–¹æ³•ï¼š
    1. è®­ç»ƒæ—¶æ­£å¸¸ä½¿ç”¨ dropout
    2. æ¨ç†æ—¶ä¿æŒ dropout æ¿€æ´»
    3. å¤šæ¬¡å‰å‘ä¼ æ’­è·å¾—åˆ†å¸ƒ
    """

    def __init__(self, config: Dict):
        super().__init__(config)
        self.num_samples = config.get("num_samples", 100)
        self.dropout_rate = config.get("dropout_rate", 0.1)

    def estimate(
        self,
        x: torch.Tensor,
        model: nn.Module,
        return_samples: bool = False
    ) -> Dict[str, torch.Tensor]:
        """
        MC Dropout ä¼°è®¡
        """
        # ç¡®ä¿æ¨¡å‹åœ¨è®­ç»ƒæ¨¡å¼ï¼ˆæ¿€æ´» dropoutï¼‰
        was_training = model.training
        model.train()

        # ç¦ç”¨æ¢¯åº¦è®¡ç®—
        with torch.no_grad():
            samples = []
            for _ in range(self.num_samples):
                logits = model(x)
                probs = torch.sigmoid(logits)
                samples.append(probs)

            samples = torch.stack(samples)  # [S, B]

        # æ¢å¤æ¨¡å‹çŠ¶æ€
        model.train(was_training)

        # è®¡ç®—ç»Ÿè®¡é‡
        mean_pred = samples.mean(dim=0)  # [B]
        epistemic = samples.var(dim=0)   # [B]

        # ä¼°è®¡å¶ç„¶ä¸ç¡®å®šæ€§ï¼ˆäºŒåˆ†ç±»çš„ä¼¯åŠªåˆ©æ–¹å·®ï¼‰
        aleatoric = mean_pred * (1 - mean_pred)

        # æ€»ä¸ç¡®å®šæ€§
        total = epistemic + aleatoric

        results = {
            'mean': mean_pred,
            'epistemic': epistemic,
            'aleatoric': aleatoric,
            'total': total,
        }

        if return_samples:
            results['samples'] = samples

        return results

    @staticmethod
    def enable_dropout(model: nn.Module):
        """
        å¯ç”¨æ¨¡å‹ä¸­çš„æ‰€æœ‰ Dropout å±‚
        """
        for module in model.modules():
            if isinstance(module, nn.Dropout):
                module.train()
```

#### 1.3 Deep Ensembles å®ç°

**æ–‡ä»¶:** `src/modules/uncertainty/ensemble.py`

```python
import torch
import torch.nn as nn
from typing import Dict, List
from .base import UncertaintyEstimator


class EnsembleEstimator(UncertaintyEstimator):
    """
    Deep Ensembles ä¸ç¡®å®šæ€§ä¼°è®¡

    ä½¿ç”¨æ–¹æ³•ï¼š
    1. è®­ç»ƒå¤šä¸ªç‹¬ç«‹åˆå§‹åŒ–çš„æ¨¡å‹
    2. æ¨ç†æ—¶èšåˆæ‰€æœ‰æ¨¡å‹çš„é¢„æµ‹
    3. ä½¿ç”¨é¢„æµ‹æ–¹å·®ä½œä¸ºä¸ç¡®å®šæ€§
    """

    def __init__(self, config: Dict):
        super().__init__(config)
        self.num_models = config.get("num_models", 5)
        self.aggregation = config.get("aggregation", "mean")
        self.models = nn.ModuleList()

    def add_model(self, model: nn.Module):
        """æ·»åŠ ä¸€ä¸ªæ¨¡å‹åˆ°é›†æˆä¸­"""
        self.models.append(model)

    def estimate(
        self,
        x: torch.Tensor,
        model: nn.Module = None,  # ä¸ä½¿ç”¨ï¼Œä¿æŒæ¥å£ä¸€è‡´
        return_samples: bool = False
    ) -> Dict[str, torch.Tensor]:
        """
        Ensemble ä¼°è®¡
        """
        if len(self.models) == 0:
            raise ValueError("æ²¡æœ‰å¯ç”¨çš„æ¨¡å‹ï¼Œè¯·å…ˆä½¿ç”¨ add_model() æ·»åŠ ")

        # æ”¶é›†æ‰€æœ‰æ¨¡å‹çš„é¢„æµ‹
        with torch.no_grad():
            predictions = []
            for m in self.models:
                m.eval()
                logits = m(x)
                probs = torch.sigmoid(logits)
                predictions.append(probs)

            predictions = torch.stack(predictions)  # [M, B]

        # èšåˆé¢„æµ‹
        if self.aggregation == "mean":
            mean_pred = predictions.mean(dim=0)
        elif self.aggregation == "median":
            mean_pred = predictions.median(dim=0)[0]
        elif self.aggregation == "weighted":
            # TODO: å®ç°åŠ æƒèšåˆ
            weights = self._compute_weights(predictions)
            mean_pred = (predictions * weights.unsqueeze(1)).sum(dim=0)
        else:
            raise ValueError(f"æœªçŸ¥çš„èšåˆæ–¹æ³•: {self.aggregation}")

        # è®¤çŸ¥ä¸ç¡®å®šæ€§ï¼ˆæ¨¡å‹é—´çš„å·®å¼‚ï¼‰
        epistemic = predictions.var(dim=0)

        # å¶ç„¶ä¸ç¡®å®šæ€§ï¼ˆå¹³å‡é¢„æµ‹çš„å†…åœ¨ä¸ç¡®å®šæ€§ï¼‰
        aleatoric = mean_pred * (1 - mean_pred)

        # æ€»ä¸ç¡®å®šæ€§
        total = epistemic + aleatoric

        results = {
            'mean': mean_pred,
            'epistemic': epistemic,
            'aleatoric': aleatoric,
            'total': total,
        }

        if return_samples:
            results['samples'] = predictions

        return results

    def _compute_weights(self, predictions: torch.Tensor) -> torch.Tensor:
        """
        è®¡ç®—æ¨¡å‹æƒé‡ï¼ˆåŸºäºå†å²æ€§èƒ½ï¼‰

        Args:
            predictions: [M, B] æ‰€æœ‰æ¨¡å‹çš„é¢„æµ‹

        Returns:
            weights: [M] å½’ä¸€åŒ–æƒé‡
        """
        # ç®€å•å®ç°ï¼šä½¿ç”¨å‡åŒ€æƒé‡
        # TODO: åŸºäºéªŒè¯é›†æ€§èƒ½è®¡ç®—æƒé‡
        num_models = predictions.shape[0]
        return torch.ones(num_models) / num_models
```

---

### Phase 2: æ ¡å‡†æ¨¡å— (Week 2)

**æ–‡ä»¶:** `src/modules/uncertainty/calibration.py`

```python
import torch
import torch.nn as nn
import torch.optim as optim
from typing import Optional


class TemperatureScaling(nn.Module):
    """
    æ¸©åº¦ç¼©æ”¾æ ¡å‡†

    ç®€å•ä½†æœ‰æ•ˆçš„æ ¡å‡†æ–¹æ³•ï¼š
    calibrated_prob = sigmoid(logit / temperature)
    """

    def __init__(self):
        super().__init__()
        self.temperature = nn.Parameter(torch.ones(1))

    def forward(self, logits: torch.Tensor) -> torch.Tensor:
        """
        åº”ç”¨æ¸©åº¦ç¼©æ”¾

        Args:
            logits: åŸå§‹ logits [N]

        Returns:
            æ ¡å‡†åçš„æ¦‚ç‡ [N]
        """
        return torch.sigmoid(logits / self.temperature)

    def fit(
        self,
        logits: torch.Tensor,
        labels: torch.Tensor,
        lr: float = 0.01,
        max_iter: int = 50
    ):
        """
        åœ¨éªŒè¯é›†ä¸Šä¼˜åŒ–æ¸©åº¦å‚æ•°

        Args:
            logits: éªŒè¯é›† logits [N]
            labels: éªŒè¯é›†æ ‡ç­¾ [N]
            lr: å­¦ä¹ ç‡
            max_iter: æœ€å¤§è¿­ä»£æ¬¡æ•°
        """
        optimizer = optim.LBFGS([self.temperature], lr=lr, max_iter=max_iter)

        def eval_loss():
            optimizer.zero_grad()
            loss = nn.BCEWithLogitsLoss()(
                logits / self.temperature,
                labels.float()
            )
            loss.backward()
            return loss

        optimizer.step(eval_loss)

        print(f"æœ€ä¼˜æ¸©åº¦: {self.temperature.item():.4f}")


class IsotonicRegression:
    """
    ä¿åºå›å½’æ ¡å‡†

    æ›´çµæ´»ä½†éœ€è¦æ›´å¤šæ•°æ®
    """

    def __init__(self):
        self.calibrator = None

    def fit(self, probs: torch.Tensor, labels: torch.Tensor):
        """
        æ‹Ÿåˆä¿åºå›å½’æ¨¡å‹
        """
        from sklearn.isotonic import IsotonicRegression as IR

        probs_np = probs.detach().cpu().numpy()
        labels_np = labels.detach().cpu().numpy()

        self.calibrator = IR(out_of_bounds='clip')
        self.calibrator.fit(probs_np, labels_np)

    def __call__(self, probs: torch.Tensor) -> torch.Tensor:
        """
        åº”ç”¨æ ¡å‡†
        """
        if self.calibrator is None:
            raise ValueError("è¯·å…ˆè°ƒç”¨ fit() æ–¹æ³•")

        probs_np = probs.detach().cpu().numpy()
        calibrated = self.calibrator.transform(probs_np)

        return torch.from_numpy(calibrated).to(probs.device)


def get_calibrator(method: str = "temperature_scaling"):
    """
    å·¥å‚å‡½æ•°ï¼šè·å–æ ¡å‡†å™¨
    """
    if method == "temperature_scaling":
        return TemperatureScaling()
    elif method == "isotonic":
        return IsotonicRegression()
    else:
        raise ValueError(f"æœªçŸ¥çš„æ ¡å‡†æ–¹æ³•: {method}")
```

---

### Phase 3: è¯„ä¼°æŒ‡æ ‡ (Week 2)

**æ–‡ä»¶:** `src/modules/uncertainty/metrics.py`

```python
import torch
import numpy as np
from typing import Tuple


def expected_calibration_error(
    probs: torch.Tensor,
    labels: torch.Tensor,
    n_bins: int = 10
) -> float:
    """
    æœŸæœ›æ ¡å‡†è¯¯å·® (ECE)

    Args:
        probs: é¢„æµ‹æ¦‚ç‡ [N]
        labels: çœŸå®æ ‡ç­¾ [N]
        n_bins: åˆ†ç®±æ•°é‡

    Returns:
        ECE å€¼
    """
    probs = probs.detach().cpu().numpy()
    labels = labels.detach().cpu().numpy()

    bin_boundaries = np.linspace(0, 1, n_bins + 1)
    bin_lowers = bin_boundaries[:-1]
    bin_uppers = bin_boundaries[1:]

    ece = 0.0
    for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):
        in_bin = (probs > bin_lower) & (probs <= bin_upper)
        prop_in_bin = in_bin.mean()

        if prop_in_bin > 0:
            accuracy_in_bin = labels[in_bin].mean()
            avg_confidence_in_bin = probs[in_bin].mean()
            ece += np.abs(avg_confidence_in_bin - accuracy_in_bin) * prop_in_bin

    return float(ece)


def brier_score(
    probs: torch.Tensor,
    labels: torch.Tensor
) -> float:
    """
    Brier åˆ†æ•°ï¼ˆè¶Šå°è¶Šå¥½ï¼‰

    BS = mean((prob - label)^2)
    """
    return ((probs - labels.float()) ** 2).mean().item()


def uncertainty_auroc(
    uncertainties: torch.Tensor,
    errors: torch.Tensor
) -> float:
    """
    ç”¨ä¸ç¡®å®šæ€§é¢„æµ‹é”™è¯¯çš„ AUROC

    Args:
        uncertainties: ä¸ç¡®å®šæ€§åˆ†æ•° [N]
        errors: æ˜¯å¦é¢„æµ‹é”™è¯¯ [N] (0 æˆ– 1)

    Returns:
        AUROC å€¼
    """
    from sklearn.metrics import roc_auc_score

    unc_np = uncertainties.detach().cpu().numpy()
    err_np = errors.detach().cpu().numpy()

    return roc_auc_score(err_np, unc_np)


def compute_all_metrics(
    probs: torch.Tensor,
    labels: torch.Tensor,
    uncertainties: torch.Tensor
) -> dict:
    """
    è®¡ç®—æ‰€æœ‰ä¸ç¡®å®šæ€§æŒ‡æ ‡
    """
    preds = (probs > 0.5).float()
    errors = (preds != labels).float()

    metrics = {
        'ece': expected_calibration_error(probs, labels),
        'brier': brier_score(probs, labels),
        'unc_auroc': uncertainty_auroc(uncertainties, errors),
    }

    return metrics
```

---

## ğŸ§ª ä½¿ç”¨ç¤ºä¾‹

### ç¤ºä¾‹ 1: MC Dropout

```python
from src.modules.uncertainty import MCDropoutEstimator

# é…ç½®
config = {
    "num_samples": 100,
    "dropout_rate": 0.1
}

# åˆ›å»ºä¼°è®¡å™¨
unc_estimator = MCDropoutEstimator(config)

# ä½¿ç”¨
results = unc_estimator(features, model)

print(f"é¢„æµ‹: {results['mean']}")
print(f"è®¤çŸ¥ä¸ç¡®å®šæ€§: {results['epistemic']}")
print(f"æ€»ä¸ç¡®å®šæ€§: {results['total']}")

# æ ¡å‡†
unc_estimator.calibrate(val_logits, val_labels)
```

### ç¤ºä¾‹ 2: Deep Ensembles

```python
from src.modules.uncertainty import EnsembleEstimator

# åˆ›å»ºensemble
config = {"num_models": 5, "aggregation": "mean"}
ensemble = EnsembleEstimator(config)

# æ·»åŠ è®­ç»ƒå¥½çš„æ¨¡å‹
for model_path in model_paths:
    model = load_model(model_path)
    ensemble.add_model(model)

# é¢„æµ‹
results = ensemble(features)
```

---

## âœ… æµ‹è¯•æ¸…å•

- [ ] å•å…ƒæµ‹è¯•ï¼šMC Dropout
- [ ] å•å…ƒæµ‹è¯•ï¼šDeep Ensembles
- [ ] å•å…ƒæµ‹è¯•ï¼šæ¸©åº¦ç¼©æ”¾
- [ ] é›†æˆæµ‹è¯•ï¼šå®Œæ•´æµç¨‹
- [ ] æ€§èƒ½æµ‹è¯•ï¼šæ¨ç†é€Ÿåº¦
- [ ] æ ¡å‡†è´¨é‡æµ‹è¯•ï¼šECE < 0.05

---

**å®ç°è€…:** UAAM-Phish Team
**ä»£ç å®¡æŸ¥:** Pending
