# Fusion (RCAF) Module - å®ç°ç»†èŠ‚

> **å®ç°ç‰ˆæœ¬:** 1.0
> **çŠ¶æ€:** è§„åˆ’ä¸­
> **è§„æ ¼æ–‡æ¡£:** [fusion_rcaf.md](../specs/fusion_rcaf.md)
> **æœ€åæ›´æ–°:** 2025-10-22

---

## ğŸ“‚ æ–‡ä»¶ç»“æ„

```
src/modules/fusion/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ rcaf.py              # RCAFä¸»æ¨¡å—
â”œâ”€â”€ attention.py         # æ³¨æ„åŠ›æœºåˆ¶
â”œâ”€â”€ gating.py            # é—¨æ§ç½‘ç»œ
â””â”€â”€ losses.py            # èåˆæŸå¤±å‡½æ•°
```

---

## ğŸ”¨ å®Œæ•´å®ç°

### RCAF Fusion å®ç°

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, List, Tuple, Optional


class RCAFFusion(nn.Module):
    """
    Reliability-Constrained Attention Fusion

    èåˆå¤šæ¨¡æ€ç‰¹å¾ï¼Œè€ƒè™‘ï¼š
    1. æ³¨æ„åŠ›æœºåˆ¶å­¦ä¹ æ¨¡æ€é‡è¦æ€§
    2. ä¸ç¡®å®šæ€§çº¦æŸæ¨¡æ€æƒé‡
    3. ä¸€è‡´æ€§çº¦æŸå¢å¼ºé²æ£’æ€§
    """

    def __init__(self, config: Dict):
        super().__init__()
        self.embedding_dim = config['embedding_dim']
        self.num_modalities = config.get('num_modalities', 3)
        self.reliability_weight = config.get('reliability_weight', 0.5)

        # æ³¨æ„åŠ›æ¨¡å—
        self.attention = nn.MultiheadAttention(
            embed_dim=self.embedding_dim,
            num_heads=config.get('num_heads', 4),
            dropout=config.get('dropout', 0.1)
        )

        # å¯é æ€§ç¼–ç å™¨
        self.reliability_encoder = nn.Sequential(
            nn.Linear(2, 32),  # [uncertainty, consistency]
            nn.ReLU(),
            nn.Linear(32, 1),
            nn.Sigmoid()
        )

        # é—¨æ§ç½‘ç»œï¼ˆå¤„ç†ç¼ºå¤±æ¨¡æ€ï¼‰
        self.gate = nn.Sequential(
            nn.Linear(self.embedding_dim, self.embedding_dim // 2),
            nn.ReLU(),
            nn.Linear(self.embedding_dim // 2, self.num_modalities),
            nn.Sigmoid()
        )

        # åˆ†ç±»å¤´
        self.classifier = nn.Sequential(
            nn.Linear(self.embedding_dim, self.embedding_dim // 2),
            nn.ReLU(),
            nn.Dropout(config.get('dropout', 0.1)),
            nn.Linear(self.embedding_dim // 2, 1)
        )

    def forward(
        self,
        embeddings: Dict[str, torch.Tensor],
        uncertainties: Optional[Dict[str, torch.Tensor]] = None,
        consistency: Optional[torch.Tensor] = None,
        mask: Optional[Dict[str, torch.Tensor]] = None
    ) -> Dict[str, torch.Tensor]:
        """
        å‰å‘ä¼ æ’­

        Args:
            embeddings: {modality: [B, D]} å„æ¨¡æ€åµŒå…¥
            uncertainties: {modality: [B]} å„æ¨¡æ€ä¸ç¡®å®šæ€§
            consistency: [B] ä¸€è‡´æ€§åˆ†æ•°
            mask: {modality: [B]} æ¨¡æ€å¯ç”¨æ€§æ©ç 

        Returns:
            èåˆç»“æœ
        """
        batch_size = list(embeddings.values())[0].shape[0]
        device = list(embeddings.values())[0].device

        # 1. å †å æ‰€æœ‰æ¨¡æ€åµŒå…¥
        modality_names = list(embeddings.keys())
        emb_list = [embeddings[mod] for mod in modality_names]
        emb_stack = torch.stack(emb_list, dim=1)  # [B, M, D]

        # 2. è®¡ç®—æ³¨æ„åŠ›æƒé‡
        # ä½¿ç”¨è‡ªæ³¨æ„åŠ›
        attn_output, attn_weights = self.attention(
            query=emb_stack.transpose(0, 1),  # [M, B, D]
            key=emb_stack.transpose(0, 1),
            value=emb_stack.transpose(0, 1)
        )
        attn_output = attn_output.transpose(0, 1)  # [B, M, D]

        # 3. è®¡ç®—å¯é æ€§æƒé‡
        if uncertainties is not None and consistency is not None:
            reliability_weights = []
            for mod in modality_names:
                unc = uncertainties.get(mod, torch.zeros(batch_size).to(device))
                cons = consistency

                # ç¼–ç å¯é æ€§
                reliability_input = torch.stack([unc, cons], dim=1)  # [B, 2]
                reliability = self.reliability_encoder(reliability_input).squeeze(1)  # [B]
                reliability_weights.append(reliability)

            reliability_weights = torch.stack(reliability_weights, dim=1)  # [B, M]
        else:
            reliability_weights = torch.ones(batch_size, len(modality_names)).to(device)

        # 4. å¤„ç†æ¨¡æ€æ©ç ï¼ˆç¼ºå¤±æ¨¡æ€ï¼‰
        if mask is not None:
            mask_tensor = torch.stack([mask.get(mod, torch.ones(batch_size).to(device))
                                      for mod in modality_names], dim=1)  # [B, M]
            reliability_weights = reliability_weights * mask_tensor

        # 5. å½’ä¸€åŒ–æƒé‡
        reliability_weights = F.softmax(reliability_weights, dim=1)  # [B, M]

        # 6. åŠ æƒèåˆ
        weighted_embeddings = attn_output * reliability_weights.unsqueeze(2)  # [B, M, D]
        fused_embedding = weighted_embeddings.sum(dim=1)  # [B, D]

        # 7. åˆ†ç±»
        logits = self.classifier(fused_embedding).squeeze(1)  # [B]
        probs = torch.sigmoid(logits)
        preds = (probs > 0.5).float()

        # 8. æ„å»ºè¾“å‡º
        return {
            'fused_embedding': fused_embedding,
            'prediction': preds,
            'probability': probs,
            'logits': logits,
            'attention_weights': {
                mod: reliability_weights[:, i]
                for i, mod in enumerate(modality_names)
            },
            'reliability_scores': {
                mod: reliability_weights[:, i]
                for i, mod in enumerate(modality_names)
            }
        }


class FusionLoss(nn.Module):
    """
    èåˆæ¨¡å‹çš„æŸå¤±å‡½æ•°
    """

    def __init__(self, config: Dict):
        super().__init__()
        self.lambda_cls = config.get('lambda_cls', 1.0)
        self.lambda_reg = config.get('lambda_reg', 0.1)
        self.lambda_div = config.get('lambda_div', 0.1)

    def forward(
        self,
        logits: torch.Tensor,
        labels: torch.Tensor,
        attention_weights: Dict[str, torch.Tensor]
    ) -> Dict[str, torch.Tensor]:
        """
        è®¡ç®—æ€»æŸå¤±
        """
        # åˆ†ç±»æŸå¤±
        cls_loss = F.binary_cross_entropy_with_logits(logits, labels.float())

        # æ³¨æ„åŠ›æ­£åˆ™åŒ–ï¼ˆé˜²æ­¢è¿‡åº¦é›†ä¸­ï¼‰
        weights_stack = torch.stack(list(attention_weights.values()), dim=1)  # [B, M]
        entropy = -(weights_stack * torch.log(weights_stack + 1e-9)).sum(dim=1).mean()
        reg_loss = -entropy  # æœ€å¤§åŒ–ç†µ = æœ€å°åŒ–è´Ÿç†µ

        # å¤šæ ·æ€§æŸå¤±ï¼ˆé¼“åŠ±ä¸åŒæ¨¡æ€å…³æ³¨ä¸åŒæ–¹é¢ï¼‰
        # TODO: å®ç°å¤šæ ·æ€§æŸå¤±
        div_loss = torch.tensor(0.0).to(logits.device)

        # æ€»æŸå¤±
        total_loss = (self.lambda_cls * cls_loss +
                     self.lambda_reg * reg_loss +
                     self.lambda_div * div_loss)

        return {
            'total': total_loss,
            'classification': cls_loss,
            'regularization': reg_loss,
            'diversity': div_loss
        }
```

---

## ğŸ§ª ä½¿ç”¨ç¤ºä¾‹

```python
# é…ç½®
config = {
    'embedding_dim': 768,
    'num_modalities': 3,
    'num_heads': 4,
    'dropout': 0.1,
    'reliability_weight': 0.5
}

# åˆ›å»ºæ¨¡å‹
fusion_model = RCAFFusion(config)

# å‰å‘ä¼ æ’­
embeddings = {
    'url': url_embeddings,    # [B, 768]
    'html': html_embeddings,  # [B, 768]
    'image': img_embeddings   # [B, 768]
}

uncertainties = {
    'url': url_uncertainty,   # [B]
    'html': html_uncertainty, # [B]
    'image': img_uncertainty  # [B]
}

consistency = consistency_scores  # [B]

results = fusion_model(embeddings, uncertainties, consistency)

# è®­ç»ƒ
loss_fn = FusionLoss(config)
losses = loss_fn(results['logits'], labels, results['attention_weights'])
losses['total'].backward()
```

---

**å®ç°è€…:** UAAM-Phish Team
