# Uncertainty Module - æŠ€æœ¯è§„æ ¼

> **æ¨¡å—åç§°:** ä¸ç¡®å®šæ€§ä¼°è®¡æ¨¡å—
> **ç‰ˆæœ¬:** 1.0
> **çŠ¶æ€:** è§„åˆ’ä¸­
> **æœ€åæ›´æ–°:** 2025-10-22

---

## ğŸ“‹ æ¦‚è¿°

ä¸ç¡®å®šæ€§ä¼°è®¡æ¨¡å—è´Ÿè´£é‡åŒ–æ¨¡å‹é¢„æµ‹çš„ä¸ç¡®å®šæ€§ï¼Œç”¨äºï¼š
- è¯†åˆ«æ¨¡å‹ä¸è‡ªä¿¡çš„é¢„æµ‹
- æä¾›å¯é æ€§è¯„åˆ†
- æ”¯æŒä¸»åŠ¨å­¦ä¹ å’Œäººå·¥å®¡æ ¸
- å¢å¼ºå¤šæ¨¡æ€èåˆçš„é²æ£’æ€§

---

## ğŸ¯ åŠŸèƒ½ç›®æ ‡

### 1. æ ¸å¿ƒåŠŸèƒ½
- **é¢„æµ‹ä¸ç¡®å®šæ€§ä¼°è®¡**ï¼šé‡åŒ–å•ä¸ªé¢„æµ‹çš„ä¸ç¡®å®šæ€§
- **å¤šæ–¹æ³•æ”¯æŒ**ï¼šMonte Carlo Dropout, Deep Ensembles, è´å¶æ–¯ç¥ç»ç½‘ç»œ
- **ä¸ç¡®å®šæ€§åˆ†è§£**ï¼šåŒºåˆ†è®¤çŸ¥ä¸ç¡®å®šæ€§å’Œå¶ç„¶ä¸ç¡®å®šæ€§
- **æ ¡å‡†**ï¼šè¾“å‡ºæ ¡å‡†åçš„ç½®ä¿¡åº¦åˆ†æ•°

### 2. è¾“å…¥è¾“å‡º

#### è¾“å…¥
```python
{
    "embeddings": Tensor[B, D],      # ç‰¹å¾åµŒå…¥
    "model": nn.Module,              # è®­ç»ƒå¥½çš„æ¨¡å‹
    "num_samples": int,              # MC é‡‡æ ·æ¬¡æ•°ï¼ˆé»˜è®¤100ï¼‰
    "method": str,                   # "mc_dropout" | "ensemble" | "bayesian"
}
```

#### è¾“å‡º
```python
{
    "predictions": Tensor[B],        # é¢„æµ‹æ ‡ç­¾
    "probabilities": Tensor[B],      # é¢„æµ‹æ¦‚ç‡
    "epistemic_unc": Tensor[B],      # è®¤çŸ¥ä¸ç¡®å®šæ€§
    "aleatoric_unc": Tensor[B],      # å¶ç„¶ä¸ç¡®å®šæ€§
    "total_unc": Tensor[B],          # æ€»ä¸ç¡®å®šæ€§
    "confidence": Tensor[B],         # æ ¡å‡†åçš„ç½®ä¿¡åº¦
}
```

---

## ğŸ“Š æ–¹æ³•è¯¦ç»†è¯´æ˜

### æ–¹æ³• 1: Monte Carlo Dropout (MC Dropout)

**åŸç†ï¼š**
- è®­ç»ƒæ—¶ä½¿ç”¨ Dropout
- æ¨ç†æ—¶ä¿æŒ Dropout æ¿€æ´»
- å¤šæ¬¡å‰å‘ä¼ æ’­è·å¾—é¢„æµ‹åˆ†å¸ƒ

**ä¼˜åŠ¿ï¼š**
- âœ… å®ç°ç®€å•
- âœ… è®¡ç®—é«˜æ•ˆ
- âœ… é€‚ç”¨äºç°æœ‰æ¨¡å‹

**å‚æ•°ï¼š**
```python
mc_dropout_config = {
    "dropout_rate": 0.1,      # Dropout æ¯”ä¾‹
    "num_samples": 100,       # MC é‡‡æ ·æ¬¡æ•°
    "use_batch_norm": False,  # æ˜¯å¦åœ¨æ¨ç†æ—¶æ›´æ–° BN
}
```

**ä¸ç¡®å®šæ€§è®¡ç®—ï¼š**
```python
# è®¤çŸ¥ä¸ç¡®å®šæ€§ï¼ˆæ¨¡å‹ä¸ç¡®å®šæ€§ï¼‰
epistemic = var(predictions)

# å¶ç„¶ä¸ç¡®å®šæ€§ï¼ˆæ•°æ®ä¸ç¡®å®šæ€§ï¼‰
aleatoric = mean(predicted_variance)

# æ€»ä¸ç¡®å®šæ€§
total = epistemic + aleatoric
```

---

### æ–¹æ³• 2: Deep Ensembles

**åŸç†ï¼š**
- è®­ç»ƒå¤šä¸ªç‹¬ç«‹æ¨¡å‹
- èšåˆé¢„æµ‹ç»“æœ
- è®¡ç®—é¢„æµ‹æ–¹å·®

**ä¼˜åŠ¿ï¼š**
- âœ… æ€§èƒ½æœ€ä½³
- âœ… ä¸ç¡®å®šæ€§ä¼°è®¡å‡†ç¡®
- âœ… æ— éœ€ç‰¹æ®Šè®­ç»ƒ

**å‚æ•°ï¼š**
```python
ensemble_config = {
    "num_models": 5,          # æ¨¡å‹æ•°é‡
    "aggregation": "mean",    # "mean" | "weighted" | "voting"
    "diversity_loss": True,   # æ˜¯å¦ä½¿ç”¨å¤šæ ·æ€§æŸå¤±
}
```

---

### æ–¹æ³• 3: Bayesian Neural Networks

**åŸç†ï¼š**
- ä½¿ç”¨è´å¶æ–¯æƒé‡
- å˜åˆ†æ¨æ–­
- åéªŒåˆ†å¸ƒé‡‡æ ·

**ä¼˜åŠ¿ï¼š**
- âœ… ç†è®ºåŸºç¡€æ‰å®
- âœ… ä¸ç¡®å®šæ€§ä¼°è®¡å‡†ç¡®

**å‚æ•°ï¼š**
```python
bayesian_config = {
    "prior_std": 0.1,         # å…ˆéªŒæ ‡å‡†å·®
    "posterior_samples": 50,  # åéªŒé‡‡æ ·æ¬¡æ•°
    "kl_weight": 0.01,        # KL æ•£åº¦æƒé‡
}
```

---

## ğŸ”§ æ¥å£è®¾è®¡

### ä¸»ç±»ï¼šUncertaintyEstimator

```python
class UncertaintyEstimator(nn.Module):
    """
    ä¸ç¡®å®šæ€§ä¼°è®¡å™¨åŸºç±»

    Args:
        method: ä¼°è®¡æ–¹æ³• ("mc_dropout" | "ensemble" | "bayesian")
        config: æ–¹æ³•ç‰¹å®šé…ç½®
    """

    def __init__(self, method: str, config: Dict):
        super().__init__()
        self.method = method
        self.config = config
        self._setup_estimator()

    def forward(self, x: Tensor, model: nn.Module) -> Dict[str, Tensor]:
        """
        å‰å‘ä¼ æ’­è®¡ç®—ä¸ç¡®å®šæ€§

        Args:
            x: è¾“å…¥å¼ é‡ [B, D]
            model: é¢„æµ‹æ¨¡å‹

        Returns:
            åŒ…å«é¢„æµ‹å’Œä¸ç¡®å®šæ€§çš„å­—å…¸
        """
        pass

    def calibrate(self, probs: Tensor, labels: Tensor) -> nn.Module:
        """
        æ ¡å‡†ç½®ä¿¡åº¦

        Args:
            probs: é¢„æµ‹æ¦‚ç‡ [N]
            labels: çœŸå®æ ‡ç­¾ [N]

        Returns:
            æ ¡å‡†æ¨¡å‹
        """
        pass
```

### MC Dropout å®ç°

```python
class MCDropoutEstimator(UncertaintyEstimator):
    def forward(self, x: Tensor, model: nn.Module) -> Dict[str, Tensor]:
        # å¯ç”¨ dropout
        model.train()

        # MC é‡‡æ ·
        predictions = []
        for _ in range(self.config["num_samples"]):
            with torch.no_grad():
                pred = model(x)
                predictions.append(pred)

        predictions = torch.stack(predictions)  # [S, B, 1]

        # è®¡ç®—ç»Ÿè®¡é‡
        mean_pred = predictions.mean(dim=0)
        epistemic = predictions.var(dim=0)

        return {
            "predictions": (mean_pred > 0.5).float(),
            "probabilities": torch.sigmoid(mean_pred),
            "epistemic_unc": epistemic,
            "total_unc": epistemic,
        }
```

### Ensemble å®ç°

```python
class EnsembleEstimator(UncertaintyEstimator):
    def __init__(self, config: Dict):
        super().__init__("ensemble", config)
        self.models = nn.ModuleList([
            self._create_model()
            for _ in range(config["num_models"])
        ])

    def forward(self, x: Tensor, model: nn.Module = None) -> Dict[str, Tensor]:
        # é›†æˆé¢„æµ‹
        predictions = []
        for m in self.models:
            m.eval()
            with torch.no_grad():
                pred = m(x)
                predictions.append(pred)

        predictions = torch.stack(predictions)  # [M, B, 1]

        # èšåˆ
        mean_pred = predictions.mean(dim=0)
        epistemic = predictions.var(dim=0)

        return {
            "predictions": (mean_pred > 0.5).float(),
            "probabilities": torch.sigmoid(mean_pred),
            "epistemic_unc": epistemic,
            "total_unc": epistemic,
        }
```

---

## ğŸ“ˆ è¯„ä¼°æŒ‡æ ‡

### 1. ä¸ç¡®å®šæ€§è´¨é‡æŒ‡æ ‡

- **ECE (Expected Calibration Error)**: æœŸæœ›æ ¡å‡†è¯¯å·®
- **NLL (Negative Log-Likelihood)**: è´Ÿå¯¹æ•°ä¼¼ç„¶
- **Brier Score**: é¢„æµ‹å‡†ç¡®æ€§

### 2. å¯é æ€§æŒ‡æ ‡

- **AUROC-uncertainty**: ç”¨ä¸ç¡®å®šæ€§é¢„æµ‹é”™è¯¯çš„èƒ½åŠ›
- **Coverage**: é«˜ç½®ä¿¡åº¦é¢„æµ‹çš„è¦†ç›–ç‡
- **Risk-coverage curve**: é£é™©-è¦†ç›–ç‡æ›²çº¿

---

## ğŸ›ï¸ é…ç½®å‚æ•°

### å…¨å±€é…ç½®

```yaml
uncertainty:
  method: mc_dropout  # mc_dropout | ensemble | bayesian
  calibration: true   # æ˜¯å¦æ ¡å‡†

  # MC Dropout é…ç½®
  mc_dropout:
    dropout_rate: 0.1
    num_samples: 100

  # Ensemble é…ç½®
  ensemble:
    num_models: 5
    aggregation: mean

  # æ ¡å‡†é…ç½®
  calibration:
    method: temperature_scaling  # temperature_scaling | isotonic
    val_size: 0.2
```

---

## ğŸ”— ä¸å…¶ä»–æ¨¡å—çš„é›†æˆ

### 1. ä¸ç¼–ç å™¨é›†æˆ

```python
# URL ç¼–ç å™¨ + ä¸ç¡®å®šæ€§
url_embedding = url_encoder(batch)
uncertainty_output = uncertainty_estimator(url_embedding, classifier)
```

### 2. ä¸èåˆæ¨¡å—é›†æˆ

```python
# æä¾›å¯é æ€§æƒé‡
fusion_weights = 1.0 / (1.0 + uncertainty_output["total_unc"])
```

### 3. ä¸ä¸€è‡´æ€§æ£€æŸ¥é›†æˆ

```python
# é«˜ä¸ç¡®å®šæ€§ + ä½ä¸€è‡´æ€§ = éœ€è¦äººå·¥å®¡æ ¸
if uncertainty > threshold and consistency < threshold:
    flag_for_review()
```

---

## ğŸ“š å‚è€ƒæ–‡çŒ®

1. **MC Dropout**: Gal & Ghahramani (2016) - "Dropout as a Bayesian Approximation"
2. **Deep Ensembles**: Lakshminarayanan et al. (2017) - "Simple and Scalable Predictive Uncertainty Estimation"
3. **Calibration**: Guo et al. (2017) - "On Calibration of Modern Neural Networks"

---

## âœ… éªŒæ”¶æ ‡å‡†

- [ ] å®ç°è‡³å°‘ä¸¤ç§ä¸ç¡®å®šæ€§ä¼°è®¡æ–¹æ³•
- [ ] ECE < 0.05 ï¼ˆæ ¡å‡†åï¼‰
- [ ] AUROC-uncertainty > 0.8
- [ ] æ¨ç†é€Ÿåº¦ < 100ms/sampleï¼ˆMC Dropoutï¼‰
- [ ] å®Œæ•´çš„å•å…ƒæµ‹è¯•è¦†ç›–
- [ ] è¯¦ç»†çš„ä½¿ç”¨æ–‡æ¡£

---

**ä½œè€…:** UAAM-Phish Team
**å®¡æ ¸:** Pending
**å®ç°æ–‡æ¡£:** [uncertainty_impl.md](../impl/uncertainty_impl.md)
