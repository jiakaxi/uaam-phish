# HTMLæ¨¡æ€é’“é±¼æ£€æµ‹ - å®Œæ•´å®æ–½æŒ‡å—

> **æ—¥æœŸ**: 2025-11-05
> **çŠ¶æ€**: âœ… ä»£ç å®Œæˆï¼Œå‡†å¤‡è®­ç»ƒ
> **ä½œè€…**: UAAM-Phish Team

---

## ğŸ“‹ ç›®å½•

1. [é¡¹ç›®æ¦‚è§ˆ](#é¡¹ç›®æ¦‚è§ˆ)
2. [æ–‡ä»¶æ¸…å•](#æ–‡ä»¶æ¸…å•)
3. [ç¯å¢ƒå‡†å¤‡](#ç¯å¢ƒå‡†å¤‡)
4. [æ•°æ®å‡†å¤‡](#æ•°æ®å‡†å¤‡)
5. [è®­ç»ƒæŒ‡å—](#è®­ç»ƒæŒ‡å—)
6. [æ•…éšœæ’é™¤](#æ•…éšœæ’é™¤)
7. [æ€§èƒ½åŸºçº¿](#æ€§èƒ½åŸºçº¿)
8. [ä¸‹ä¸€æ­¥](#ä¸‹ä¸€æ­¥)

---

## é¡¹ç›®æ¦‚è§ˆ

### ğŸ¯ ç›®æ ‡

å®ç°åŸºäºBERTçš„HTMLå†…å®¹é’“é±¼æ£€æµ‹ï¼Œä½œä¸ºå¤šæ¨¡æ€ç³»ç»Ÿçš„é‡è¦ç»„æˆéƒ¨åˆ†ã€‚

### âœ… å·²å®ŒæˆåŠŸèƒ½

- **HTMLEncoder**: BERT-baseç¼–ç å™¨ï¼ˆ110Må‚æ•°ï¼‰
- **HtmlDataset**: æ”¯æŒBERT tokenizationçš„æ•°æ®é›†
- **HtmlDataModule**: Lightningæ•°æ®æ¨¡å—ï¼Œæ”¯æŒä¸‰ç§åè®®
- **HtmlOnlyModule**: å®Œæ•´çš„è®­ç»ƒç³»ç»Ÿ
- **é…ç½®æ–‡ä»¶**: Hydraé…ç½®ï¼Œå¼€ç®±å³ç”¨

### ğŸ—ï¸ æ¶æ„è®¾è®¡

```
HTMLæ–‡ä»¶ â†’ clean_html() â†’ çº¯æ–‡æœ¬
    â†“
BERT tokenizer â†’ (input_ids, attention_mask)
    â†“
BERT-base â†’ [CLS] token (768ç»´)
    â†“
æŠ•å½±å±‚ â†’ 256ç»´ï¼ˆä¸URLç¼–ç å™¨å¯¹é½ï¼‰
    â†“
åˆ†ç±»å¤´ â†’ logit â†’ BCEWithLogitsLoss
```

**å…³é”®è®¾è®¡åŸåˆ™**:
- è¾“å‡º256ç»´ï¼Œä¸URLEncoderå¯¹é½ï¼ˆæœªæ¥èåˆï¼‰
- æ”¯æŒfreeze_berté€‰é¡¹ï¼ˆèŠ‚çœæ˜¾å­˜å’Œè®­ç»ƒæ—¶é—´ï¼‰
- å®Œæ•´çš„metricså’Œartifactsç”Ÿæˆ
- ä¸‰ç§æ•°æ®åˆ†å‰²åè®®æ”¯æŒ

---

## æ–‡ä»¶æ¸…å•

### ğŸ“ æ ¸å¿ƒä»£ç æ–‡ä»¶

| æ–‡ä»¶ | è¡Œæ•° | åŠŸèƒ½ | çŠ¶æ€ |
|------|------|------|------|
| `src/models/html_encoder.py` | 86 | BERTç¼–ç å™¨ | âœ… å®Œæˆ |
| `src/data/html_dataset.py` | 111 | Datasetç±» | âœ… å®Œæˆ |
| `src/datamodules/html_datamodule.py` | 152 | DataModule | âœ… å®Œæˆ |
| `src/systems/html_only_module.py` | 291 | Lightningæ¨¡å— | âœ… å®Œæˆ |
| `src/utils/html_clean.py` | 76 | HTMLæ¸…æ´—å·¥å…· | âœ… å®Œæˆ |

### ğŸ“ é…ç½®æ–‡ä»¶

| æ–‡ä»¶ | åŠŸèƒ½ | å…³é”®å‚æ•° |
|------|------|----------|
| `configs/model/html_encoder.yaml` | æ¨¡å‹é…ç½® | bert_model, dropout, freeze_bert |
| `configs/data/html_only.yaml` | æ•°æ®é…ç½® | html_max_len=512 |
| `configs/experiment/html_baseline.yaml` | å®éªŒé…ç½® | lr=2e-5, bs=32 |

### ğŸ“Š ä»£ç ç»“æ„

```
src/
â”œâ”€â”€ models/
â”‚   â””â”€â”€ html_encoder.py          # HTMLEncoderç±»
â”œâ”€â”€ data/
â”‚   â””â”€â”€ html_dataset.py          # HtmlDatasetç±»
â”œâ”€â”€ datamodules/
â”‚   â””â”€â”€ html_datamodule.py       # HtmlDataModuleç±»
â”œâ”€â”€ systems/
â”‚   â””â”€â”€ html_only_module.py      # HtmlOnlyModuleç±»
â””â”€â”€ utils/
    â””â”€â”€ html_clean.py            # clean_html(), load_html_from_path()

configs/
â”œâ”€â”€ model/
â”‚   â””â”€â”€ html_encoder.yaml        # æ¨¡å‹è¶…å‚æ•°
â”œâ”€â”€ data/
â”‚   â””â”€â”€ html_only.yaml           # æ•°æ®è·¯å¾„
â””â”€â”€ experiment/
    â””â”€â”€ html_baseline.yaml       # å®Œæ•´å®éªŒé…ç½®
```

---

## ç¯å¢ƒå‡†å¤‡

### 1. ä¾èµ–å®‰è£…

```bash
# æ ¸å¿ƒä¾èµ–
pip install transformers>=4.30.0
pip install beautifulsoup4>=4.11.0
pip install lxml>=4.9.0

# æˆ–ä½¿ç”¨requirements.txt
pip install -r requirements.txt
```

### 2. éªŒè¯å®‰è£…

```bash
# æ£€æŸ¥transformers
python -c "from transformers import AutoModel, AutoTokenizer; print('âœ… transformers OK')"

# æ£€æŸ¥BeautifulSoup
python -c "from bs4 import BeautifulSoup; print('âœ… beautifulsoup4 OK')"

# æ£€æŸ¥BERTæ¨¡å‹ï¼ˆé¦–æ¬¡ä¼šä¸‹è½½ï¼‰
python -c "
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')
print('âœ… BERT model OK')
"
```

### 3. ä¸‹è½½BERTæ¨¡å‹ï¼ˆå¯é€‰ï¼Œé¦–æ¬¡è®­ç»ƒä¼šè‡ªåŠ¨ä¸‹è½½ï¼‰

```bash
# é¢„ä¸‹è½½BERT-baseï¼ˆçº¦440MBï¼‰
python -c "
from transformers import AutoModel, AutoTokenizer
AutoModel.from_pretrained('bert-base-uncased')
AutoTokenizer.from_pretrained('bert-base-uncased')
print('âœ… BERT-base downloaded')
"

# é¢„ä¸‹è½½DistilBERTï¼ˆçº¦260MBï¼Œæ›´å¿«ï¼‰
python -c "
from transformers import AutoModel, AutoTokenizer
AutoModel.from_pretrained('distilbert-base-uncased')
AutoTokenizer.from_pretrained('distilbert-base-uncased')
print('âœ… DistilBERT downloaded')
"
```

---

## æ•°æ®å‡†å¤‡

### 1. æ•°æ®æ ¼å¼è¦æ±‚

HTMLé¡¹ç›®éœ€è¦ä»¥ä¸‹æ•°æ®ï¼š

```csv
# master_v2.csv å¿…éœ€åˆ—
url_text,html_path,label,timestamp,brand,source
https://example.com,data/html/sample1.html,0,2024-01-01,legitimate,dataset_a
https://phish.com,data/html/sample2.html,1,2024-01-02,paypal,dataset_b
```

**å¿…éœ€å­—æ®µ**:
- `html_path`: HTMLæ–‡ä»¶è·¯å¾„ï¼ˆç›¸å¯¹æˆ–ç»å¯¹ï¼‰
- `label`: æ ‡ç­¾ï¼ˆ0=benign, 1=phishingï¼‰

**å¯é€‰å­—æ®µ** (åè®®éœ€è¦):
- `timestamp`: æ—¶é—´æˆ³ï¼ˆtemporalåè®®ï¼‰
- `brand`: å“ç‰Œåç§°ï¼ˆbrand_oodåè®®ï¼‰
- `source`: æ•°æ®æºï¼ˆç»Ÿè®¡ç”¨ï¼‰

### 2. æ•°æ®éªŒè¯

```bash
# éªŒè¯CSVæ ¼å¼
python -c "
import pandas as pd
from pathlib import Path

df = pd.read_csv('data/processed/master_v2.csv')
print('âœ… æ€»æ ·æœ¬æ•°:', len(df))
print('âœ… HTMLåˆ—å­˜åœ¨:', 'html_path' in df.columns)
print('âœ… æ ‡ç­¾åˆ†å¸ƒ:', df['label'].value_counts().to_dict())

# éªŒè¯HTMLæ–‡ä»¶
html_exists = df['html_path'].apply(lambda x: Path(x).exists()).sum()
print(f'âœ… HTMLæ–‡ä»¶å­˜åœ¨: {html_exists}/{len(df)}')
"
```

### 3. æ•°æ®å‡çº§ï¼ˆå¦‚æœéœ€è¦ï¼‰

å¦‚æœç°æœ‰æ•°æ®é›†ç¼ºå°‘å¿…éœ€å­—æ®µï¼Œè¿è¡Œå‡çº§è„šæœ¬ï¼š

```bash
python scripts/upgrade_dataset.py \
  --input data/processed/master.csv \
  --output data/processed/master_v2.csv
```

è¿™å°†è‡ªåŠ¨ï¼š
- æ·»åŠ `brand`å’Œ`timestamp`å­—æ®µ
- ä»HTML/URLæå–å“ç‰Œä¿¡æ¯
- ç”Ÿæˆåˆç†çš„æ—¶é—´æˆ³

### 4. æ£€æŸ¥HTMLæ–‡ä»¶

```bash
# æ£€æŸ¥HTMLæ–‡ä»¶å¯è¯»æ€§
python -c "
from src.utils.html_clean import load_html_from_path, clean_html
html_path = 'data/processed/html/sample.html'
html_text = load_html_from_path(html_path)
clean_text = clean_html(html_text)
print('åŸå§‹é•¿åº¦:', len(html_text))
print('æ¸…æ´—åé•¿åº¦:', len(clean_text))
print('å‰100å­—ç¬¦:', clean_text[:100])
"
```

---

## è®­ç»ƒæŒ‡å—

### ğŸš€ å¿«é€Ÿå¼€å§‹ï¼ˆ5åˆ†é’ŸéªŒè¯ï¼‰

```bash
# æœ€å°æµ‹è¯• - éªŒè¯æµç¨‹
python scripts/train_hydra.py \
  experiment=html_baseline \
  trainer=local \
  data.sample_fraction=0.05 \
  train.epochs=2 \
  model.freeze_bert=true \
  run.name=html_smoke_test

# é¢„æœŸè¾“å‡ºï¼š
# Epoch 1/2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| ... loss=0.xxx val/auroc=0.xxx
# Epoch 2/2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| ... loss=0.xxx val/auroc=0.xxx
# âœ… Saved: experiments/html_smoke_test/results/
```

### ğŸ“Š æ ‡å‡†è®­ç»ƒï¼ˆæ¨èï¼‰

#### æ–¹æ¡ˆ1: DistilBERTï¼ˆæ¨èï¼Œæ›´å¿«ï¼‰

```bash
python scripts/train_hydra.py \
  experiment=html_baseline \
  model.bert_model=distilbert-base-uncased \
  model.hidden_dim=768 \
  trainer=server \
  logger=wandb \
  run.name=html_distilbert_baseline
```

**ä¼˜åŠ¿**:
- å‚æ•°é‡66Mï¼ˆBERT-baseçš„60%ï¼‰
- è®­ç»ƒé€Ÿåº¦å¿«2å€
- æ˜¾å­˜éœ€æ±‚ä½30%
- æ€§èƒ½æŸå¤±<2%

#### æ–¹æ¡ˆ2: BERT-baseï¼ˆæœ€å¼ºæ€§èƒ½ï¼‰

```bash
python scripts/train_hydra.py \
  experiment=html_baseline \
  model.bert_model=bert-base-uncased \
  trainer=server \
  logger=wandb \
  hardware.precision=16-mixed \
  run.name=html_bert_baseline
```

**ä¼˜åŠ¿**:
- å‚æ•°é‡110M
- æœ€ä½³æ€§èƒ½
- æ›´å¥½çš„æ ¡å‡†

### ğŸ”¬ ä¸‰ç§åè®®è®­ç»ƒ

```bash
# 1. Randomåè®®ï¼ˆé»˜è®¤ï¼‰
python scripts/train_hydra.py \
  experiment=html_baseline \
  protocol=random \
  run.name=html_random

# 2. Temporalåè®®ï¼ˆæ—¶é—´åºåˆ—ï¼‰
python scripts/train_hydra.py \
  experiment=html_baseline \
  protocol=temporal \
  run.name=html_temporal

# 3. Brand-OODåè®®ï¼ˆå“ç‰Œæ³›åŒ–ï¼‰
python scripts/train_hydra.py \
  experiment=html_baseline \
  protocol=brand_ood \
  run.name=html_brand_ood
```

### ğŸ¯ è¶…å‚æ•°è°ƒä¼˜

#### å­¦ä¹ ç‡æœç´¢

```bash
python scripts/train_hydra.py -m \
  experiment=html_baseline \
  train.lr=1e-5,2e-5,5e-5,1e-4 \
  run.name=html_lr_search
```

#### Batch Sizeè°ƒä¼˜

```bash
python scripts/train_hydra.py -m \
  experiment=html_baseline \
  train.bs=16,32,64 \
  run.name=html_bs_search
```

#### Freeze BERTå¯¹æ¯”

```bash
python scripts/train_hydra.py -m \
  experiment=html_baseline \
  model.freeze_bert=true,false \
  run.name=html_freeze_compare
```

### ğŸ“ æŸ¥çœ‹ç»“æœ

```bash
# æŸ¥çœ‹æœ€æ–°å®éªŒ
python scripts/compare_experiments.py --latest 1

# å¯¹æ¯”å¤šä¸ªå®éªŒ
python scripts/compare_experiments.py --latest 5

# æ‰¾åˆ°æœ€ä½³æ¨¡å‹
python scripts/compare_experiments.py --find_best --metric auroc
```

---

## æ•…éšœæ’é™¤

### é—®é¢˜1: ModuleNotFoundError: transformers

**ç—‡çŠ¶**:
```
ModuleNotFoundError: No module named 'transformers'
```

**è§£å†³**:
```bash
pip install transformers>=4.30.0
# éªŒè¯
python -c "import transformers; print(transformers.__version__)"
```

### é—®é¢˜2: ModuleNotFoundError: bs4

**ç—‡çŠ¶**:
```
ModuleNotFoundError: No module named 'bs4'
```

**è§£å†³**:
```bash
pip install beautifulsoup4 lxml
# éªŒè¯
python -c "from bs4 import BeautifulSoup; print('OK')"
```

### é—®é¢˜3: HTMLæ–‡ä»¶è·¯å¾„é”™è¯¯

**ç—‡çŠ¶**:
```
FileNotFoundError: [Errno 2] No such file or directory: 'data/html/xxx.html'
```

**è¯Šæ–­**:
```bash
# æ£€æŸ¥è·¯å¾„
python -c "
import pandas as pd
from pathlib import Path
df = pd.read_csv('data/processed/master_v2.csv')
exists = df['html_path'].apply(lambda x: Path(x).exists())
print(f'å­˜åœ¨: {exists.sum()}/{len(df)}')
print('ç¬¬ä¸€ä¸ªä¸å­˜åœ¨çš„è·¯å¾„:', df.loc[~exists, 'html_path'].iloc[0])
"
```

**è§£å†³**:
1. ç¡®è®¤HTMLæ–‡ä»¶å­˜åœ¨
2. æ£€æŸ¥è·¯å¾„æ˜¯å¦ç›¸å¯¹äºé¡¹ç›®æ ¹ç›®å½•
3. é‡æ–°è¿è¡Œæ•°æ®å‡†å¤‡è„šæœ¬

### é—®é¢˜4: CUDA OOMï¼ˆæ˜¾å­˜ä¸è¶³ï¼‰

**ç—‡çŠ¶**:
```
torch.cuda.OutOfMemoryError: CUDA out of memory
```

**è§£å†³æ–¹æ¡ˆ**:

#### æ–¹æ¡ˆA: é™ä½batch size
```bash
python scripts/train_hydra.py experiment=html_baseline train.bs=16
```

#### æ–¹æ¡ˆB: ä½¿ç”¨DistilBERT
```bash
python scripts/train_hydra.py experiment=html_baseline \
  model.bert_model=distilbert-base-uncased
```

#### æ–¹æ¡ˆC: å†»ç»“BERTï¼ˆæ¨èï¼‰
```bash
python scripts/train_hydra.py experiment=html_baseline \
  model.freeze_bert=true
```

è¿™å°†ï¼š
- èŠ‚çœ50%æ˜¾å­˜
- åŠ é€Ÿè®­ç»ƒ2-3å€
- æ€§èƒ½æŸå¤±çº¦3-5%

#### æ–¹æ¡ˆD: æ¢¯åº¦ç´¯ç§¯
```bash
python scripts/train_hydra.py experiment=html_baseline \
  trainer.accumulate_grad_batches=2 \
  train.bs=16
```
ç­‰æ•ˆäºbs=32ï¼Œä½†æ˜¾å­˜éœ€æ±‚å‡åŠã€‚

#### æ–¹æ¡ˆE: CPUè®­ç»ƒï¼ˆæœ€åæ‰‹æ®µï¼‰
```bash
python scripts/train_hydra.py experiment=html_baseline \
  hardware.accelerator=cpu \
  train.bs=8
```

### é—®é¢˜5: BERTæ¨¡å‹ä¸‹è½½æ…¢

**ç—‡çŠ¶**:
```
Downloading: 100%|â–ˆâ–ˆ| 440M/440M [slow...]
```

**è§£å†³**:
```bash
# æ–¹æ¡ˆ1: ä½¿ç”¨é•œåƒï¼ˆä¸­å›½ç”¨æˆ·ï¼‰
export HF_ENDPOINT=https://hf-mirror.com
python scripts/train_hydra.py experiment=html_baseline

# æ–¹æ¡ˆ2: ç¦»çº¿ä¸‹è½½
# 1. æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹åˆ° ~/.cache/huggingface/
# 2. æˆ–ä½¿ç”¨æœ¬åœ°è·¯å¾„
python scripts/train_hydra.py experiment=html_baseline \
  model.bert_model=/path/to/local/bert-base-uncased
```

### é—®é¢˜6: HTMLæ¸…æ´—è¿‡æ…¢

**ç—‡çŠ¶**:
æ•°æ®åŠ è½½é€Ÿåº¦æ…¢ï¼ˆæ¯ä¸ªæ ·æœ¬>1ç§’ï¼‰

**è§£å†³**:
```bash
# æ–¹æ¡ˆ1: å‡å°‘max_len
python scripts/train_hydra.py experiment=html_baseline \
  data.html_max_len=256  # é»˜è®¤512

# æ–¹æ¡ˆ2: å¢åŠ workers
python scripts/train_hydra.py experiment=html_baseline \
  data.num_workers=8  # é»˜è®¤4

# æ–¹æ¡ˆ3: ä½¿ç”¨SSDå­˜å‚¨HTMLæ–‡ä»¶
```

---

## æ€§èƒ½åŸºçº¿

### é¢„æœŸæŒ‡æ ‡

åŸºäºè®ºæ–‡å’Œç±»ä¼¼å·¥ä½œçš„é¢„æœŸæ€§èƒ½ï¼š

| æŒ‡æ ‡ | DistilBERT | BERT-base | Freeze BERT | è¯´æ˜ |
|------|-----------|-----------|-------------|------|
| **AUROC** | 0.92-0.94 | 0.94-0.96 | 0.91-0.93 | HTMLè¯­ä¹‰ç‰¹å¾å¼º |
| **Accuracy** | 0.88-0.91 | 0.90-0.93 | 0.87-0.90 | ä¾èµ–æ•°æ®é›†è´¨é‡ |
| **F1-macro** | 0.87-0.90 | 0.89-0.92 | 0.86-0.89 | å¹³è¡¡ä¸¤ç±» |
| **NLL** | 0.20-0.30 | 0.18-0.25 | 0.22-0.32 | BERTæ ¡å‡†è¾ƒå¥½ |
| **ECE** | 0.03-0.06 | 0.02-0.05 | 0.04-0.07 | éœ€å…³æ³¨è¿‡æ‹Ÿåˆ |

### è®­ç»ƒæ—¶é—´

åŸºäºRTX 3090ï¼ˆ24GBï¼‰çš„é¢„ä¼°ï¼š

| é…ç½® | Epochs | æ—¶é—´ | æ˜¾å­˜ |
|------|--------|------|------|
| BERT-base (bs=32, fp16) | 50 | 3-4å°æ—¶ | ~8GB |
| DistilBERT (bs=32, fp16) | 50 | 2å°æ—¶ | ~6GB |
| Freeze BERT (bs=32, fp16) | 50 | 1å°æ—¶ | ~4GB |
| BERT-base (bs=16, fp32) | 50 | 6å°æ—¶ | ~12GB |

### ç¡¬ä»¶å»ºè®®

| é…ç½® | æœ€ä½ | æ¨è | æœ€ä½³ |
|------|------|------|------|
| **GPU** | GTX 1060 6GB | RTX 3060 12GB | RTX 3090 24GB |
| **CPU** | 4æ ¸ | 8æ ¸ | 16æ ¸ |
| **RAM** | 16GB | 32GB | 64GB |
| **å­˜å‚¨** | HDD | SSD | NVMe SSD |

**é…ç½®å»ºè®®**:
- 6GBæ˜¾å­˜: freeze_bert=true, bs=16
- 12GBæ˜¾å­˜: DistilBERT, bs=32
- 24GBæ˜¾å­˜: BERT-base, bs=64

---

## ä¸‹ä¸€æ­¥

### âœ… éªŒè¯æ¸…å•

è®­ç»ƒå‰è¯·ç¡®è®¤ï¼š

- [ ] **ç¯å¢ƒå‡†å¤‡**
  - [ ] transformers, beautifulsoup4å·²å®‰è£…
  - [ ] BERTæ¨¡å‹å·²ä¸‹è½½ï¼ˆæˆ–ç½‘ç»œå¯è®¿é—®ï¼‰
  - [ ] GPUé©±åŠ¨å’ŒCUDAæ­£å¸¸

- [ ] **æ•°æ®å‡†å¤‡**
  - [ ] master_v2.csvå­˜åœ¨ä¸”æ ¼å¼æ­£ç¡®
  - [ ] html_pathåˆ—è·¯å¾„æ­£ç¡®
  - [ ] HTMLæ–‡ä»¶å¯è®¿é—®
  - [ ] æ ‡ç­¾åˆ†å¸ƒåˆç†

- [ ] **é…ç½®æ£€æŸ¥**
  - [ ] experiment=html_baselineå­˜åœ¨
  - [ ] batch_sizeé€‚é…æ˜¾å­˜
  - [ ] å­¦ä¹ ç‡åœ¨åˆç†èŒƒå›´ï¼ˆ1e-5åˆ°5e-5ï¼‰

### ğŸ“… å®æ–½è®¡åˆ’

#### ä»Šå¤©ï¼ˆ1å°æ—¶ï¼‰

```bash
# 1. ç¯å¢ƒéªŒè¯ï¼ˆ10åˆ†é’Ÿï¼‰
pip install transformers beautifulsoup4 lxml
python -c "from transformers import AutoModel; print('OK')"

# 2. æ•°æ®éªŒè¯ï¼ˆ10åˆ†é’Ÿï¼‰
python -c "
import pandas as pd
df = pd.read_csv('data/processed/master_v2.csv')
print('Samples:', len(df))
print('HTML:', 'html_path' in df.columns)
"

# 3. å¿«é€Ÿæµ‹è¯•ï¼ˆ5åˆ†é’Ÿï¼‰
python scripts/train_hydra.py \
  experiment=html_baseline \
  trainer=local \
  data.sample_fraction=0.05 \
  train.epochs=2 \
  model.freeze_bert=true

# 4. æŸ¥çœ‹ç»“æœï¼ˆ5åˆ†é’Ÿï¼‰
python scripts/compare_experiments.py --latest 1
```

#### æœ¬å‘¨ï¼ˆ10å°æ—¶ï¼‰

1. **Day 1-2: åŸºçº¿è®­ç»ƒ**
   - Randomåè®®å®Œæ•´è®­ç»ƒ
   - è®°å½•baselineæ€§èƒ½
   - éªŒè¯artifactsç”Ÿæˆ

2. **Day 3-4: è¶…å‚æ•°è°ƒä¼˜**
   - å­¦ä¹ ç‡æœç´¢ï¼ˆ1e-5, 2e-5, 5e-5ï¼‰
   - Batch sizeä¼˜åŒ–ï¼ˆ16, 32, 64ï¼‰
   - Freezeç­–ç•¥å¯¹æ¯”

3. **Day 5: åè®®å¯¹æ¯”**
   - Temporalåè®®è®­ç»ƒ
   - Brand-OODåè®®è®­ç»ƒ
   - æ€§èƒ½å¯¹æ¯”åˆ†æ

4. **Day 6-7: ç»“æœåˆ†æ**
   - é”™è¯¯æ¡ˆä¾‹åˆ†æ
   - ä¸URLæ¨¡å‹å¯¹æ¯”
   - æ’°å†™å®éªŒæŠ¥å‘Š

#### æœ¬æœˆï¼ˆ40å°æ—¶ï¼‰

1. **Week 1: åŸºçº¿å»ºç«‹**
   - ä¸‰ç§åè®®å®Œæ•´è®­ç»ƒ
   - DistilBERT vs BERTå¯¹æ¯”
   - æ€§èƒ½åŸºçº¿ç¡®ç«‹

2. **Week 2: æ¨¡å‹ä¼˜åŒ–**
   - è¶…å‚æ•°ç²¾ç»†è°ƒä¼˜
   - æ··åˆç²¾åº¦è®­ç»ƒ
   - æ¨¡å‹é›†æˆæ¢ç´¢

3. **Week 3: æ·±åº¦åˆ†æ**
   - BERT attentionå¯è§†åŒ–
   - é”™è¯¯åˆ†æå’Œæ”¹è¿›
   - HTMLç‰¹å¾é‡è¦æ€§

4. **Week 4: æ–‡æ¡£æ•´ç†**
   - å®éªŒæŠ¥å‘Šæ’°å†™
   - æœ€ä½³å®è·µæ€»ç»“
   - è®ºæ–‡/æŠ¥å‘Šå‡†å¤‡

### ğŸ¯ æˆåŠŸæ ‡å‡†

HTMLæ¨¡å‹è¾¾åˆ°ä»¥ä¸‹æ ‡å‡†å³ä¸ºæˆåŠŸï¼š

âœ… **åŸºç¡€æ€§èƒ½**
- AUROC â‰¥ 0.90
- Accuracy â‰¥ 0.85
- F1-macro â‰¥ 0.84

âœ… **æ ¡å‡†è´¨é‡**
- ECE â‰¤ 0.10
- NLL â‰¤ 0.40

âœ… **é²æ£’æ€§**
- ä¸‰ç§åè®®å‡å¯è®­ç»ƒ
- æ€§èƒ½æ ‡å‡†å·® < 0.02
- æ— æ•°æ®æ³„éœ²

âœ… **å¯å¤ç°æ€§**
- é…ç½®å®Œæ•´ä¿å­˜
- éšæœºç§å­å›ºå®š
- å®éªŒå¯é‡å¤

âœ… **å·¥ç¨‹è´¨é‡**
- æ— runtimeé”™è¯¯
- 4ä¸ªartifactså®Œæ•´
- WandBæ—¥å¿—å®Œæ•´

---

## é™„å½•

### A. é…ç½®å‚æ•°è¯¦è§£

#### æ¨¡å‹å‚æ•° (configs/model/html_encoder.yaml)

```yaml
model:
  bert_model: bert-base-uncased  # æˆ– distilbert-base-uncased
  hidden_dim: 768                # BERTè¾“å‡ºç»´åº¦ï¼ˆå›ºå®šï¼‰
  output_dim: 256                # æŠ•å½±ç»´åº¦ï¼ˆå¿…é¡»256ï¼Œèåˆéœ€è¦ï¼‰
  dropout: 0.1                   # Dropoutç‡
  freeze_bert: false             # æ˜¯å¦å†»ç»“BERTå‚æ•°
```

**å‚æ•°è¯´æ˜**:
- `bert_model`: å¯é€‰bert-base-uncased, distilbert-base-uncased, roberta-base
- `freeze_bert=true`: èŠ‚çœ50%æ˜¾å­˜ï¼ŒåŠ é€Ÿ2-3å€ï¼Œæ€§èƒ½æŸå¤±3-5%
- `output_dim`: **ä¸è¦ä¿®æ”¹**ï¼Œå¿…é¡»ä¿æŒ256ä»¥ä¾¿æœªæ¥èåˆ

#### æ•°æ®å‚æ•° (configs/data/html_only.yaml)

```yaml
data:
  html_max_len: 512      # BERTæœ€å¤§tokenæ•°ï¼ˆå»ºè®®256-512ï¼‰
  num_workers: 4         # DataLoader workersï¼ˆå»ºè®®4-8ï¼‰
  batch_format: tuple    # ä¸è¦ä¿®æ”¹
```

#### è®­ç»ƒå‚æ•° (configs/experiment/html_baseline.yaml)

```yaml
train:
  epochs: 50             # è®­ç»ƒè½®æ•°
  lr: 2.0e-5            # å­¦ä¹ ç‡ï¼ˆBERTå¸¸ç”¨1e-5åˆ°5e-5ï¼‰
  bs: 32                # Batch sizeï¼ˆæ ¹æ®æ˜¾å­˜è°ƒæ•´ï¼‰
  weight_decay: 0.01    # æƒé‡è¡°å‡ï¼ˆL2æ­£åˆ™ï¼‰
```

### B. å‘½ä»¤è¡Œå‚æ•°é€ŸæŸ¥

```bash
# æ¨¡å‹ç›¸å…³
model.bert_model=distilbert-base-uncased
model.freeze_bert=true
model.dropout=0.2

# æ•°æ®ç›¸å…³
data.sample_fraction=0.1        # ä½¿ç”¨10%æ•°æ®
data.html_max_len=256           # å‡å°‘tokené•¿åº¦
data.num_workers=8              # å¢åŠ workers

# è®­ç»ƒç›¸å…³
train.epochs=100
train.lr=5e-5
train.bs=64
train.weight_decay=0.01

# ç¡¬ä»¶ç›¸å…³
hardware.accelerator=gpu
hardware.devices=2              # å¤šGPU
hardware.precision=16-mixed     # æ··åˆç²¾åº¦

# åè®®ç›¸å…³
protocol=random                 # æˆ– temporal, brand_ood
use_build_splits=true

# æ—¥å¿—ç›¸å…³
logger=wandb
run.name=my_experiment
run.tags=[html,baseline,v1]
```

### C. æ–‡ä»¶è·¯å¾„çº¦å®š

```
project_root/
â”œâ”€â”€ data/
â”‚   â””â”€â”€ processed/
â”‚       â”œâ”€â”€ master_v2.csv              # ä¸»CSV
â”‚       â”œâ”€â”€ html_train_v2.csv          # è®­ç»ƒé›†
â”‚       â”œâ”€â”€ html_val_v2.csv            # éªŒè¯é›†
â”‚       â”œâ”€â”€ html_test_v2.csv           # æµ‹è¯•é›†
â”‚       â””â”€â”€ html/                      # HTMLæ–‡ä»¶ç›®å½•
â”‚           â”œâ”€â”€ benign_001.html
â”‚           â”œâ”€â”€ phish_001.html
â”‚           â””â”€â”€ ...
â”œâ”€â”€ experiments/
â”‚   â””â”€â”€ <run_name>/
â”‚       â”œâ”€â”€ config.yaml                # å®Œæ•´é…ç½®
â”‚       â”œâ”€â”€ checkpoints/               # æ¨¡å‹æ£€æŸ¥ç‚¹
â”‚       â”‚   â””â”€â”€ best_model.ckpt
â”‚       â”œâ”€â”€ results/                   # ç»“æœartifacts
â”‚       â”‚   â”œâ”€â”€ roc_random.png
â”‚       â”‚   â”œâ”€â”€ calib_random.png
â”‚       â”‚   â”œâ”€â”€ splits_random.csv
â”‚       â”‚   â””â”€â”€ metrics_random.json
â”‚       â””â”€â”€ logs/
â”‚           â””â”€â”€ train.log
â””â”€â”€ configs/
    â””â”€â”€ experiment/
        â””â”€â”€ html_baseline.yaml
```

---

## ğŸ”— ç›¸å…³èµ„æº

- **ä¸»æ–‡æ¡£**: `FINAL_SUMMARY_CN.md`
- **è®ºæ–‡å‚è€ƒ**: Thesis Â§3.3 (HTML Encoder Architecture)
- **ä»£ç ç¤ºä¾‹**: `src/systems/html_only_module.py`
- **é…ç½®ç¤ºä¾‹**: `configs/experiment/html_baseline.yaml`
- **æ•°æ®å‡†å¤‡**: `scripts/upgrade_dataset.py`
- **HTMLæ¸…æ´—**: `src/utils/html_clean.py`

---

## ğŸ“ è·å–å¸®åŠ©

é‡åˆ°é—®é¢˜ï¼Ÿ

1. æŸ¥çœ‹æœ¬æ–‡æ¡£çš„[æ•…éšœæ’é™¤](#æ•…éšœæ’é™¤)éƒ¨åˆ†
2. æ£€æŸ¥`experiments/<run>/logs/train.log`
3. æŸ¥çœ‹WandBå®éªŒé¡µé¢
4. æäº¤Issueå¹¶é™„ä¸Šï¼š
   - å®Œæ•´é”™è¯¯ä¿¡æ¯
   - è¿è¡Œå‘½ä»¤
   - ç¯å¢ƒä¿¡æ¯ï¼ˆGPUå‹å·ï¼ŒPythonç‰ˆæœ¬ç­‰ï¼‰

---

**ç¥HTMLæ¨¡å‹è®­ç»ƒé¡ºåˆ©ï¼** ğŸš€

---

*æœ€åæ›´æ–°: 2025-11-05*
