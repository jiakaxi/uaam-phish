#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å…¨é¢æ£€æŸ¥æ•°æ®é›†çš„é‡å¤å’Œç¼ºå¤±æƒ…å†µ
"""

import sys
import pandas as pd
from pathlib import Path

# Handle Windows console encoding
if sys.platform == "win32":
    try:
        sys.stdout.reconfigure(encoding="utf-8")
        sys.stderr.reconfigure(encoding="utf-8")
    except AttributeError:
        import codecs

        sys.stdout = codecs.getwriter("utf-8")(sys.stdout.buffer, "strict")
        sys.stderr = codecs.getwriter("utf-8")(sys.stderr.buffer, "strict")


def check_duplicates(df: pd.DataFrame):
    """æ£€æŸ¥å„ç§ç±»å‹çš„é‡å¤"""
    print("\n" + "=" * 70)
    print("ğŸ” é‡å¤æ£€æŸ¥")
    print("=" * 70)

    # 1. IDé‡å¤
    id_dups = df["id"].duplicated().sum()
    print("\n1. ID é‡å¤:")
    print(f"   é‡å¤æ•°é‡: {id_dups}")
    if id_dups > 0:
        dup_ids = df[df["id"].duplicated(keep=False)]["id"].value_counts()
        print("   é‡å¤çš„ID (å‰10ä¸ª):")
        for id_val, count in dup_ids.head(10).items():
            print(f"     - {id_val}: {count}æ¬¡")

    # 2. URLé‡å¤
    url_dups = df["url_text"].duplicated().sum()
    print("\n2. URL é‡å¤:")
    print(f"   é‡å¤æ•°é‡: {url_dups}")
    if url_dups > 0:
        dup_urls = df[df["url_text"].duplicated(keep=False)]["url_text"].value_counts()
        print("   é‡å¤çš„URL (å‰5ä¸ª):")
        for url, count in dup_urls.head(5).items():
            print(f"     - {url[:80]}...: {count}æ¬¡")

    # 3. HTMLè·¯å¾„é‡å¤
    html_dups = df["html_path"].duplicated().sum()
    print("\n3. HTMLè·¯å¾„ é‡å¤:")
    print(f"   é‡å¤æ•°é‡: {html_dups}")
    if html_dups > 0:
        dup_htmls = df[df["html_path"].duplicated(keep=False)][
            "html_path"
        ].value_counts()
        print("   é‡å¤çš„HTMLè·¯å¾„ (å‰5ä¸ª):")
        for path, count in dup_htmls.head(5).items():
            print(f"     - {path}: {count}æ¬¡")

    # 4. IMGè·¯å¾„é‡å¤
    img_dups = df["img_path"].duplicated().sum()
    print("\n4. IMGè·¯å¾„ é‡å¤:")
    print(f"   é‡å¤æ•°é‡: {img_dups}")
    if img_dups > 0:
        dup_imgs = df[df["img_path"].duplicated(keep=False)]["img_path"].value_counts()
        print("   é‡å¤çš„IMGè·¯å¾„ (å‰5ä¸ª):")
        for path, count in dup_imgs.head(5).items():
            print(f"     - {path}: {count}æ¬¡")

    # 5. è¯­ä¹‰é‡å¤ (URL + domain + brand)
    df["semantic_key"] = (
        df["url_text"].astype(str)
        + "|"
        + df["domain"].astype(str)
        + "|"
        + df["brand"].astype(str)
    )
    semantic_dups = df["semantic_key"].duplicated().sum()
    print("\n5. è¯­ä¹‰é‡å¤ (URL+domain+brand):")
    print(f"   é‡å¤æ•°é‡: {semantic_dups}")
    if semantic_dups > 0:
        dup_semantic = df[df["semantic_key"].duplicated(keep=False)][
            ["url_text", "domain", "brand"]
        ].head(5)
        print("   é‡å¤çš„è¯­ä¹‰ç»„åˆ (å‰5ä¸ª):")
        for idx, row in dup_semantic.iterrows():
            print(f"     - URL: {row['url_text'][:60]}...")
            print(f"       Domain: {row['domain']}, Brand: {row['brand']}")

    # 6. å®Œå…¨ç›¸åŒçš„è¡Œï¼ˆæ‰€æœ‰åˆ—éƒ½ç›¸åŒï¼‰
    full_dups = df.duplicated().sum()
    print("\n6. å®Œå…¨é‡å¤çš„è¡Œ:")
    print(f"   é‡å¤æ•°é‡: {full_dups}")

    return {
        "id": id_dups,
        "url": url_dups,
        "html_path": html_dups,
        "img_path": img_dups,
        "semantic": semantic_dups,
        "full_row": full_dups,
    }


def check_missing(df: pd.DataFrame):
    """æ£€æŸ¥ç¼ºå¤±å€¼"""
    print("\n" + "=" * 70)
    print("ğŸ” ç¼ºå¤±å€¼æ£€æŸ¥")
    print("=" * 70)

    total = len(df)

    for col in df.columns:
        missing = df[col].isna().sum()
        missing_pct = (missing / total) * 100

        if missing > 0:
            print(f"\nåˆ— '{col}':")
            print(f"   ç¼ºå¤±æ•°é‡: {missing} / {total} ({missing_pct:.2f}%)")

            # æ˜¾ç¤ºç¼ºå¤±å€¼çš„æ ·æœ¬ID
            if missing <= 10:
                missing_ids = df[df[col].isna()]["id"].tolist()
                print(f"   ç¼ºå¤±çš„æ ·æœ¬ID: {missing_ids}")

    # æ£€æŸ¥ç©ºå­—ç¬¦ä¸²ï¼ˆéNaNä½†ä¸ºç©ºï¼‰
    print("\nç©ºå­—ç¬¦ä¸²æ£€æŸ¥:")
    for col in ["url_text", "domain", "brand", "source"]:
        if col in df.columns:
            empty = (df[col] == "").sum() if df[col].dtype == "object" else 0
            if empty > 0:
                print(f"   åˆ— '{col}': {empty} ä¸ªç©ºå­—ç¬¦ä¸²")


def check_path_validity(df: pd.DataFrame, sample_size: int = 100):
    """æ£€æŸ¥è·¯å¾„æœ‰æ•ˆæ€§"""
    print("\n" + "=" * 70)
    print("ğŸ” è·¯å¾„æœ‰æ•ˆæ€§æ£€æŸ¥")
    print("=" * 70)

    # é‡‡æ ·æ£€æŸ¥
    sample_df = df.sample(n=min(sample_size, len(df)), random_state=42)

    # HTMLè·¯å¾„
    html_exists = 0
    html_missing = 0
    for path_str in sample_df["html_path"]:
        if pd.isna(path_str):
            html_missing += 1
        elif Path(path_str).exists():
            html_exists += 1
        else:
            html_missing += 1

    print(f"\nHTMLè·¯å¾„ (é‡‡æ · {len(sample_df)} ä¸ª):")
    print(f"   å­˜åœ¨: {html_exists} ({html_exists/len(sample_df)*100:.1f}%)")
    print(f"   ç¼ºå¤±: {html_missing} ({html_missing/len(sample_df)*100:.1f}%)")

    # IMGè·¯å¾„
    img_exists = 0
    img_missing = 0
    for path_str in sample_df["img_path"]:
        if pd.isna(path_str):
            img_missing += 1
        elif Path(path_str).exists():
            img_exists += 1
        else:
            img_missing += 1

    print(f"\nIMGè·¯å¾„ (é‡‡æ · {len(sample_df)} ä¸ª):")
    print(f"   å­˜åœ¨: {img_exists} ({img_exists/len(sample_df)*100:.1f}%)")
    print(f"   ç¼ºå¤±: {img_missing} ({img_missing/len(sample_df)*100:.1f}%)")


def check_data_consistency(df: pd.DataFrame):
    """æ£€æŸ¥æ•°æ®ä¸€è‡´æ€§"""
    print("\n" + "=" * 70)
    print("ğŸ” æ•°æ®ä¸€è‡´æ€§æ£€æŸ¥")
    print("=" * 70)

    # 1. æ ‡ç­¾å€¼æ£€æŸ¥
    label_values = df["label"].unique()
    print("\n1. æ ‡ç­¾å€¼:")
    print(f"   å”¯ä¸€å€¼: {sorted(label_values)}")
    print(f"   åˆ†å¸ƒ: {df['label'].value_counts().to_dict()}")
    invalid_labels = df[~df["label"].isin([0, 1])]
    if len(invalid_labels) > 0:
        print(f"   âš ï¸  å‘ç°æ— æ•ˆæ ‡ç­¾: {len(invalid_labels)} ä¸ª")

    # 2. Splitå€¼æ£€æŸ¥
    split_values = df["split"].unique()
    print("\n2. Splitå€¼:")
    print(f"   å”¯ä¸€å€¼: {sorted(split_values)}")
    print(f"   åˆ†å¸ƒ: {df['split'].value_counts().to_dict()}")

    # 3. æ—¶é—´æˆ³æ ¼å¼æ£€æŸ¥
    print("\n3. æ—¶é—´æˆ³æ ¼å¼:")
    ts_sample = df["timestamp"].dropna().sample(n=min(5, len(df)), random_state=42)
    print("   æ ·æœ¬ (å‰5ä¸ª):")
    for ts in ts_sample:
        print(f"     - {ts}")

    # æ£€æŸ¥æ˜¯å¦æœ‰æ— æ•ˆçš„æ—¶é—´æˆ³
    try:
        pd.to_datetime(df["timestamp"], errors="coerce")
        invalid_ts = pd.to_datetime(df["timestamp"], errors="coerce").isna().sum()
        print(f"   æ— æ•ˆæ—¶é—´æˆ³: {invalid_ts}")
    except Exception as e:
        print(f"   æ—¶é—´æˆ³è§£æé”™è¯¯: {e}")

    # 4. å“ç‰Œåˆ†å¸ƒ
    print("\n4. å“ç‰Œåˆ†å¸ƒ:")
    brand_counts = df["brand"].value_counts()
    print(f"   æ€»å“ç‰Œæ•°: {len(brand_counts)}")
    print("   Top 10 å“ç‰Œ:")
    for brand, count in brand_counts.head(10).items():
        pct = count / len(df) * 100
        print(f"     - {brand}: {count} ({pct:.2f}%)")

    # 5. Domainåˆ†å¸ƒ
    print("\n5. Domainåˆ†å¸ƒ:")
    domain_counts = df["domain"].value_counts()
    print(f"   æ€»åŸŸåæ•°: {len(domain_counts)}")
    print("   Top 10 åŸŸå:")
    for domain, count in domain_counts.head(10).items():
        pct = count / len(df) * 100
        print(f"     - {domain}: {count} ({pct:.2f}%)")


def main():
    csv_path = Path("data/processed/master_v2.csv")

    if not csv_path.exists():
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {csv_path}")
        return 1

    print("=" * 70)
    print(f"ğŸ“– è¯»å–æ•°æ®é›†: {csv_path}")
    print("=" * 70)

    df = pd.read_csv(csv_path)
    print(f"æ€»æ ·æœ¬æ•°: {len(df)}")
    print(f"æ€»åˆ—æ•°: {len(df.columns)}")
    print(f"åˆ—å: {list(df.columns)}")

    # æ‰§è¡Œæ£€æŸ¥
    dup_stats = check_duplicates(df)
    check_missing(df)
    check_path_validity(df, sample_size=200)
    check_data_consistency(df)

    # æ€»ç»“
    print("\n" + "=" * 70)
    print("ğŸ“Š æ£€æŸ¥æ€»ç»“")
    print("=" * 70)

    total_issues = sum(dup_stats.values())

    if total_issues == 0:
        print("\nâœ… æœªå‘ç°é‡å¤é—®é¢˜ï¼")
    else:
        print(f"\nâš ï¸  å‘ç° {total_issues} ä¸ªé‡å¤é¡¹:")
        for key, count in dup_stats.items():
            if count > 0:
                print(f"   - {key}: {count}")

    print("\n" + "=" * 70)

    return 0


if __name__ == "__main__":
    exit(main())
