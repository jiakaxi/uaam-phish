#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è¯¦ç»†åˆ†æé‡å¤é¡¹å¹¶ç”Ÿæˆæ¸…ç†å»ºè®®
"""

import sys
import pandas as pd
from pathlib import Path

# Handle Windows console encoding
if sys.platform == "win32":
    try:
        sys.stdout.reconfigure(encoding="utf-8")
        sys.stderr.reconfigure(encoding="utf-8")
    except AttributeError:
        import codecs

        sys.stdout = codecs.getwriter("utf-8")(sys.stdout.buffer, "strict")
        sys.stderr = codecs.getwriter("utf-8")(sys.stderr.buffer, "strict")


def analyze_url_duplicates(df: pd.DataFrame):
    """åˆ†æURLé‡å¤çš„è¯¦ç»†æƒ…å†µ"""
    print("\n" + "=" * 70)
    print("ğŸ“‹ URLé‡å¤è¯¦ç»†åˆ†æ")
    print("=" * 70)

    dup_urls = df[df["url_text"].duplicated(keep=False)].copy()
    dup_urls = dup_urls.sort_values("url_text")

    print(f"\næ€»é‡å¤URLæ ·æœ¬æ•°: {len(dup_urls)}")
    print(f"æ¶‰åŠçš„å”¯ä¸€URLæ•°: {dup_urls['url_text'].nunique()}")

    # æŒ‰URLåˆ†ç»„æŸ¥çœ‹
    url_groups = dup_urls.groupby("url_text")

    print("\né‡å¤URLè¯¦æƒ… (å‰10ä¸ª):")
    for i, (url, group) in enumerate(url_groups):
        if i >= 10:
            break
        print(f"\n{i+1}. URL: {url[:100]}...")
        print(f"   å‡ºç°æ¬¡æ•°: {len(group)}")
        print(f"   æ ‡ç­¾åˆ†å¸ƒ: {group['label'].value_counts().to_dict()}")
        print(f"   å“ç‰Œ: {group['brand'].unique()[:3]}")
        print(f"   æ¥æº: {group['source'].unique()[:3]}")
        print(f"   æ ·æœ¬ID: {list(group['id'][:3])}")

    return dup_urls


def analyze_missing_data(df: pd.DataFrame):
    """åˆ†æç¼ºå¤±æ•°æ®"""
    print("\n" + "=" * 70)
    print("ğŸ“‹ ç¼ºå¤±æ•°æ®è¯¦ç»†åˆ†æ")
    print("=" * 70)

    # é—®é¢˜æ ·æœ¬ï¼šæœ‰å¤šä¸ªç¼ºå¤±å€¼çš„è¡Œ
    problem_samples = df[
        df["url_text"].isna()
        | df["html_path"].isna()
        | df["img_path"].isna()
        | df["domain"].isna()
        | df["timestamp"].isna()
    ]

    print(f"\né—®é¢˜æ ·æœ¬æ€»æ•°: {len(problem_samples)}")

    if len(problem_samples) > 0:
        print("\né—®é¢˜æ ·æœ¬è¯¦æƒ…:")
        for idx, row in problem_samples.iterrows():
            print(f"\næ ·æœ¬ID: {row['id']}")
            print(f"  æ ‡ç­¾: {row['label']}")
            print(f"  æ¥æº: {row['source']}")
            print(f"  Split: {row['split']}")
            missing_cols = []
            if pd.isna(row["url_text"]):
                missing_cols.append("url_text")
            if pd.isna(row["html_path"]):
                missing_cols.append("html_path")
            if pd.isna(row["img_path"]):
                missing_cols.append("img_path")
            if pd.isna(row["domain"]):
                missing_cols.append("domain")
            if pd.isna(row["timestamp"]):
                missing_cols.append("timestamp")
            print(f"  ç¼ºå¤±åˆ—: {missing_cols}")

    return problem_samples


def suggest_cleanup(
    df: pd.DataFrame, dup_urls: pd.DataFrame, problem_samples: pd.DataFrame
):
    """ç”Ÿæˆæ¸…ç†å»ºè®®"""
    print("\n" + "=" * 70)
    print("ğŸ’¡ æ•°æ®æ¸…ç†å»ºè®®")
    print("=" * 70)

    print("\n1. URLé‡å¤å¤„ç†å»ºè®®:")
    if len(dup_urls) > 0:
        # æ£€æŸ¥æ˜¯å¦æ˜¯åˆæ³•çš„é‡å¤ï¼ˆæ¯”å¦‚ç›¸åŒURLä½†ä¸åŒæ ‡ç­¾ï¼‰
        url_groups = dup_urls.groupby("url_text")
        same_label_count = 0
        diff_label_count = 0

        for url, group in url_groups:
            if group["label"].nunique() == 1:
                same_label_count += 1
            else:
                diff_label_count += 1

        print(f"   - ç›¸åŒURL+ç›¸åŒæ ‡ç­¾: {same_label_count} ä¸ªURL (åº”åˆ é™¤é‡å¤)")
        print(f"   - ç›¸åŒURL+ä¸åŒæ ‡ç­¾: {diff_label_count} ä¸ªURL (éœ€äººå·¥æ£€æŸ¥)")

        if same_label_count > 0:
            print("\n   å»ºè®®æ“ä½œ: ä¿ç•™æ¯ä¸ªURLçš„ç¬¬ä¸€ä¸ªæ ·æœ¬ï¼Œåˆ é™¤å…¶ä½™é‡å¤")
            # è®¡ç®—å°†åˆ é™¤çš„æ ·æœ¬æ•°
            to_remove = 0
            for url, group in url_groups:
                if group["label"].nunique() == 1:
                    to_remove += len(group) - 1
            print(f"   é¢„è®¡åˆ é™¤æ ·æœ¬æ•°: {to_remove}")
    else:
        print("   âœ… æ— URLé‡å¤é—®é¢˜")

    print("\n2. ç¼ºå¤±æ•°æ®å¤„ç†å»ºè®®:")
    if len(problem_samples) > 0:
        print(f"   - å‘ç° {len(problem_samples)} ä¸ªé—®é¢˜æ ·æœ¬")
        print(f"   - æ ·æœ¬ID: {list(problem_samples['id'])}")
        print("   å»ºè®®æ“ä½œ: åˆ é™¤è¿™äº›æ ·æœ¬ï¼ˆå…³é”®å­—æ®µç¼ºå¤±ï¼Œæ— æ³•ä½¿ç”¨ï¼‰")
    else:
        print("   âœ… æ— ä¸¥é‡ç¼ºå¤±é—®é¢˜")

    print("\n3. è·¯å¾„é‡å¤å¤„ç†å»ºè®®:")
    html_path_dups = df["html_path"].duplicated().sum()
    img_path_dups = df["img_path"].duplicated().sum()
    if html_path_dups > 0 or img_path_dups > 0:
        print(f"   - HTMLè·¯å¾„é‡å¤: {html_path_dups}")
        print(f"   - IMGè·¯å¾„é‡å¤: {img_path_dups}")
        print("   å»ºè®®: ä¿ç•™ç¬¬ä¸€ä¸ªï¼Œåˆ é™¤åç»­é‡å¤ï¼ˆé¿å…æ•°æ®æ³„éœ²ï¼‰")
    else:
        print("   âœ… æ— è·¯å¾„é‡å¤é—®é¢˜ï¼ˆå¿½ç•¥å°‘é‡é‡å¤æ˜¯å®‰å…¨çš„ï¼‰")

    print("\n4. æ—¶é—´æˆ³é—®é¢˜:")
    invalid_ts = pd.to_datetime(df["timestamp"], errors="coerce").isna().sum()
    if invalid_ts > 0:
        print(f"   - å‘ç° {invalid_ts} ä¸ªæ— æ•ˆæ—¶é—´æˆ³")
        print("   è¯´æ˜: è¿™äº›å¯èƒ½æ˜¯æ—§æ•°æ®é›†çš„æ—¶é—´æˆ³æ ¼å¼é—®é¢˜")
        print("   å»ºè®®: å¦‚æœä¸å½±å“temporalåè®®ï¼Œå¯ä¿ç•™ï¼›å¦åˆ™éœ€è¦ä¿®å¤")

    print("\n5. æ€»ä½“å»ºè®®:")
    total_to_remove = len(problem_samples)
    # è®¡ç®—URLé‡å¤ä¸­åŒæ ‡ç­¾çš„é‡å¤æ•°
    if len(dup_urls) > 0:
        url_groups = dup_urls.groupby("url_text")
        for url, group in url_groups:
            if group["label"].nunique() == 1:
                total_to_remove += len(group) - 1

    print(f"   - é¢„è®¡éœ€è¦åˆ é™¤: {total_to_remove} ä¸ªæ ·æœ¬")
    print(f"   - æ¸…ç†åæ ·æœ¬æ•°: {len(df) - total_to_remove}")
    print(f"   - æ•°æ®è´¨é‡æ”¹å–„: {(total_to_remove/len(df)*100):.2f}% çš„é—®é¢˜æ ·æœ¬å°†è¢«ç§»é™¤")


def main():
    csv_path = Path("data/processed/master_v2.csv")

    print("=" * 70)
    print(f"ğŸ“– è¯»å–æ•°æ®é›†: {csv_path}")
    print("=" * 70)

    df = pd.read_csv(csv_path)
    print(f"æ€»æ ·æœ¬æ•°: {len(df)}")

    # åˆ†æ
    dup_urls = analyze_url_duplicates(df)
    problem_samples = analyze_missing_data(df)
    suggest_cleanup(df, dup_urls, problem_samples)

    print("\n" + "=" * 70)
    print("âœ… åˆ†æå®Œæˆ")
    print("=" * 70)

    return 0


if __name__ == "__main__":
    exit(main())
