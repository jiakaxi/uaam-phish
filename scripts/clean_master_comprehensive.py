#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç»¼åˆæ¸…ç† master_v2.csv æ•°æ®é›†
æŒ‰ç…§æ•°æ®è´¨é‡æŠ¥å‘Šçš„6ä¸ªé—®é¢˜ä¾æ¬¡å¤„ç†
"""

import sys
import pandas as pd
from pathlib import Path
import hashlib
import json
from datetime import datetime
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor
import re

# Handle Windows console encoding
if sys.platform == "win32":
    try:
        sys.stdout.reconfigure(encoding="utf-8")
        sys.stderr.reconfigure(encoding="utf-8")
    except AttributeError:
        import codecs

        sys.stdout = codecs.getwriter("utf-8")(sys.stdout.buffer, "strict")
        sys.stderr = codecs.getwriter("utf-8")(sys.stderr.buffer, "strict")


def compute_file_hash(file_path: Path) -> str:
    """è®¡ç®—æ–‡ä»¶SHA1å“ˆå¸Œ"""
    if not file_path.exists():
        return None
    try:
        sha1 = hashlib.sha1()
        with open(file_path, "rb") as f:
            while chunk := f.read(8192):
                sha1.update(chunk)
        return sha1.hexdigest()
    except Exception:
        return None


def compute_hashes_parallel(paths, max_workers=4):
    """å¹¶è¡Œè®¡ç®—å“ˆå¸Œ"""
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        hashes = list(
            tqdm(
                executor.map(compute_file_hash, paths),
                total=len(paths),
                desc="Computing hashes",
            )
        )
    return hashes


def fix_timestamp(ts_str):
    """
    ä¿®å¤æ—¶é—´æˆ³æ ¼å¼ï¼Œå°è¯•å¤šç§è§£ææ–¹å¼
    """
    if pd.isna(ts_str):
        return None

    ts_str = str(ts_str).strip()

    # å¦‚æœå·²ç»æ˜¯æ ‡å‡†ISOæ ¼å¼ï¼Œç›´æ¥è¿”å›
    if re.match(r"\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}", ts_str):
        # ç¡®ä¿æœ‰æ—¶åŒºä¿¡æ¯
        if not ts_str.endswith("Z") and "+" not in ts_str and not ts_str.endswith(")"):
            ts_str += "Z"
        return ts_str

    # å°è¯•å¤šç§æ ¼å¼è§£æ
    formats = [
        "%Y-%m-%d %H:%M:%S",
        "%Y/%m/%d %H:%M:%S",
        "%Y-%m-%d",
        "%Y/%m/%d",
        "%Y%m%d",
        "%Y-%m-%dT%H:%M:%S",
        "%Y-%m-%dT%H:%M:%S.%f",
    ]

    for fmt in formats:
        try:
            dt = datetime.strptime(ts_str, fmt)
            return dt.strftime("%Y-%m-%dT%H:%M:%SZ")
        except Exception:  # noqa: E722
            continue

    # å°è¯•pandasè§£æ
    try:
        dt = pd.to_datetime(ts_str)
        if pd.notna(dt):
            return dt.strftime("%Y-%m-%dT%H:%M:%SZ")
    except Exception:  # noqa: E722
        pass

    # æ— æ³•è§£æï¼Œè¿”å›åŸå€¼
    return ts_str


def main():
    print("=" * 70)
    print("ğŸ§¹ ç»¼åˆæ•°æ®æ¸…ç† - master_v2.csv")
    print("=" * 70)

    # è¯»å–æ•°æ®
    csv_path = Path("data/processed/master_v2.csv")
    backup_path = Path("data/processed/master_v2_backup.csv")
    output_path = Path("data/processed/master_v2.csv")

    if not csv_path.exists():
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {csv_path}")
        return 1

    # å¤‡ä»½åŸæ–‡ä»¶
    print("\nğŸ“¦ å¤‡ä»½åŸæ–‡ä»¶...")
    import shutil

    shutil.copy2(csv_path, backup_path)
    print(f"   å¤‡ä»½ä¿å­˜åˆ°: {backup_path}")

    print("\nğŸ“– è¯»å–æ•°æ®é›†...")
    df = pd.read_csv(csv_path)
    original_count = len(df)
    print(f"   åŸå§‹æ ·æœ¬æ•°: {original_count}")

    # è®°å½•åˆ é™¤çš„æ ·æœ¬
    removed_samples = {
        "url_duplicates": [],
        "missing_critical": [],
        "path_duplicates": [],
        "metadata_missing": [],
    }

    # ========================================================================
    # é—®é¢˜1: åˆ é™¤URLé‡å¤ï¼ˆä¿ç•™ç¬¬ä¸€ä¸ªï¼‰
    # ========================================================================
    print("\n" + "=" * 70)
    print("ğŸ”§ é—®é¢˜1: åˆ é™¤URLé‡å¤")
    print("=" * 70)

    # æ‰¾å‡ºåŒURL+åŒæ ‡ç­¾çš„é‡å¤
    url_dup_mask = df.duplicated(subset=["url_text", "label"], keep="first")
    url_dups = df[url_dup_mask]

    print(f"   å‘ç° {len(url_dups)} ä¸ªURLé‡å¤æ ·æœ¬")
    if len(url_dups) > 0:
        removed_samples["url_duplicates"] = url_dups["id"].tolist()
        print(f"   åˆ é™¤æ ·æœ¬ID (å‰10ä¸ª): {url_dups['id'].head(10).tolist()}")
        df = df[~url_dup_mask]
        print(f"   âœ… åˆ é™¤å®Œæˆï¼Œå‰©ä½™: {len(df)} ä¸ªæ ·æœ¬")

    # ========================================================================
    # é—®é¢˜2: åˆ é™¤å…³é”®å­—æ®µç¼ºå¤±çš„æ ·æœ¬
    # ========================================================================
    print("\n" + "=" * 70)
    print("ğŸ”§ é—®é¢˜2: åˆ é™¤å…³é”®å­—æ®µç¼ºå¤±æ ·æœ¬")
    print("=" * 70)

    critical_fields = ["url_text", "html_path", "img_path", "domain", "timestamp"]
    missing_mask = df[critical_fields].isna().any(axis=1)
    missing_samples = df[missing_mask]

    print(f"   å‘ç° {len(missing_samples)} ä¸ªå…³é”®å­—æ®µç¼ºå¤±æ ·æœ¬")
    if len(missing_samples) > 0:
        removed_samples["missing_critical"] = missing_samples["id"].tolist()
        print(f"   åˆ é™¤æ ·æœ¬ID: {missing_samples['id'].tolist()}")
        df = df[~missing_mask]
        print(f"   âœ… åˆ é™¤å®Œæˆï¼Œå‰©ä½™: {len(df)} ä¸ªæ ·æœ¬")

    # ========================================================================
    # é—®é¢˜3: åˆ é™¤è·¯å¾„é‡å¤ï¼ˆä¿ç•™ç¬¬ä¸€ä¸ªï¼‰
    # ========================================================================
    print("\n" + "=" * 70)
    print("ğŸ”§ é—®é¢˜3: åˆ é™¤è·¯å¾„é‡å¤")
    print("=" * 70)

    # HTMLè·¯å¾„é‡å¤
    html_dup_mask = df.duplicated(subset=["html_path"], keep="first")

    # IMGè·¯å¾„é‡å¤
    img_dup_mask = df.duplicated(subset=["img_path"], keep="first")

    path_dup_mask = html_dup_mask | img_dup_mask
    path_dups = df[path_dup_mask]

    print(f"   å‘ç° {len(path_dups)} ä¸ªè·¯å¾„é‡å¤æ ·æœ¬")
    print(f"     - HTMLè·¯å¾„é‡å¤: {html_dup_mask.sum()}")
    print(f"     - IMGè·¯å¾„é‡å¤: {img_dup_mask.sum()}")

    if len(path_dups) > 0:
        removed_samples["path_duplicates"] = path_dups["id"].tolist()
        print(f"   åˆ é™¤æ ·æœ¬ID: {path_dups['id'].tolist()}")
        df = df[~path_dup_mask]
        print(f"   âœ… åˆ é™¤å®Œæˆï¼Œå‰©ä½™: {len(df)} ä¸ªæ ·æœ¬")

    # ========================================================================
    # é—®é¢˜4: ä¿®å¤æ—¶é—´æˆ³æ ¼å¼
    # ========================================================================
    print("\n" + "=" * 70)
    print("ğŸ”§ é—®é¢˜4: ä¿®å¤æ—¶é—´æˆ³æ ¼å¼")
    print("=" * 70)

    print("   å¼€å§‹ä¿®å¤æ—¶é—´æˆ³...")
    df["timestamp_original"] = df["timestamp"].copy()

    # åº”ç”¨ä¿®å¤å‡½æ•°
    tqdm.pandas(desc="Fixing timestamps")
    df["timestamp"] = df["timestamp"].progress_apply(fix_timestamp)

    # éªŒè¯ä¿®å¤ç»“æœ
    valid_ts = pd.to_datetime(df["timestamp"], errors="coerce").notna().sum()
    invalid_ts = len(df) - valid_ts

    print("   ä¿®å¤åç»Ÿè®¡:")
    print(f"     - æœ‰æ•ˆæ—¶é—´æˆ³: {valid_ts} ({valid_ts/len(df)*100:.1f}%)")
    print(f"     - æ— æ•ˆæ—¶é—´æˆ³: {invalid_ts} ({invalid_ts/len(df)*100:.1f}%)")

    if invalid_ts > 0:
        print(f"   âš ï¸  ä»æœ‰ {invalid_ts} ä¸ªæ—¶é—´æˆ³æ— æ³•è§£æ")
        invalid_examples = df[pd.to_datetime(df["timestamp"], errors="coerce").isna()][
            "timestamp"
        ].head(5)
        print(f"   ç¤ºä¾‹: {list(invalid_examples)}")
    else:
        print("   âœ… æ‰€æœ‰æ—¶é—´æˆ³æ ¼å¼æ­£ç¡®")

    # ========================================================================
    # é—®é¢˜5: åˆ é™¤å…ƒæ•°æ®åˆ—ç¼ºå¤±çš„æ ·æœ¬
    # ========================================================================
    print("\n" + "=" * 70)
    print("ğŸ”§ é—®é¢˜5: åˆ é™¤å…ƒæ•°æ®åˆ—å®Œå…¨ç¼ºå¤±çš„æ ·æœ¬")
    print("=" * 70)

    metadata_fields = ["domain_source", "timestamp_source", "folder"]
    metadata_missing_mask = df[metadata_fields].isna().all(axis=1)
    metadata_missing = df[metadata_missing_mask]

    print(f"   å‘ç° {len(metadata_missing)} ä¸ªå…ƒæ•°æ®å®Œå…¨ç¼ºå¤±æ ·æœ¬")
    if len(metadata_missing) > 0:
        removed_samples["metadata_missing"] = metadata_missing["id"].tolist()
        print(f"   åˆ é™¤æ ·æœ¬æ•°: {len(metadata_missing)}")
        print("   (è¿™äº›æ˜¯æ—§æ•°æ®é›†æ ·æœ¬ï¼Œç¼ºå°‘æ–°æ„å»ºè„šæœ¬æ·»åŠ çš„å…ƒæ•°æ®)")
        df = df[~metadata_missing_mask]
        print(f"   âœ… åˆ é™¤å®Œæˆï¼Œå‰©ä½™: {len(df)} ä¸ªæ ·æœ¬")

    # ========================================================================
    # é—®é¢˜6: é‡æ–°è®¡ç®—å“ˆå¸Œ
    # ========================================================================
    print("\n" + "=" * 70)
    print("ğŸ”§ é—®é¢˜6: é‡æ–°è®¡ç®—æ–‡ä»¶å“ˆå¸Œ")
    print("=" * 70)

    print(f"   å‡†å¤‡è®¡ç®— {len(df)} ä¸ªæ ·æœ¬çš„å“ˆå¸Œ...")

    # HTMLå“ˆå¸Œ
    print("\n   è®¡ç®—HTMLæ–‡ä»¶å“ˆå¸Œ...")
    html_paths = [Path(p) for p in df["html_path"]]
    df["html_sha1"] = compute_hashes_parallel(html_paths, max_workers=8)
    html_success = df["html_sha1"].notna().sum()
    print(f"   âœ… æˆåŠŸ: {html_success}/{len(df)} ({html_success/len(df)*100:.1f}%)")

    # IMGå“ˆå¸Œ
    print("\n   è®¡ç®—IMGæ–‡ä»¶å“ˆå¸Œ...")
    img_paths = [Path(p) for p in df["img_path"]]
    df["img_sha1"] = compute_hashes_parallel(img_paths, max_workers=8)
    img_success = df["img_sha1"].notna().sum()
    print(f"   âœ… æˆåŠŸ: {img_success}/{len(df)} ({img_success/len(df)*100:.1f}%)")

    # ========================================================================
    # ä¿å­˜æ¸…ç†åçš„æ•°æ®
    # ========================================================================
    print("\n" + "=" * 70)
    print("ğŸ’¾ ä¿å­˜æ¸…ç†åçš„æ•°æ®")
    print("=" * 70)

    # é‡æ–°æ’åºåˆ—ï¼ˆæŠŠåŸå§‹æ—¶é—´æˆ³ç§»åˆ°æœ€åï¼‰
    cols = [c for c in df.columns if c != "timestamp_original"] + ["timestamp_original"]
    df = df[cols]

    df.to_csv(output_path, index=False, encoding="utf-8")
    print(f"   âœ… ä¿å­˜åˆ°: {output_path}")
    print(f"   æœ€ç»ˆæ ·æœ¬æ•°: {len(df)}")

    # ä¿å­˜åˆ é™¤è®°å½•
    removed_log_path = Path("data/processed/removed_samples_log.json")
    removed_summary = {
        "timestamp": datetime.now().isoformat(),
        "original_count": original_count,
        "final_count": len(df),
        "removed_count": original_count - len(df),
        "removed_by_reason": {
            "url_duplicates": len(removed_samples["url_duplicates"]),
            "missing_critical": len(removed_samples["missing_critical"]),
            "path_duplicates": len(removed_samples["path_duplicates"]),
            "metadata_missing": len(removed_samples["metadata_missing"]),
        },
        "removed_sample_ids": removed_samples,
    }

    with open(removed_log_path, "w", encoding="utf-8") as f:
        json.dump(removed_summary, f, indent=2, ensure_ascii=False)
    print(f"   âœ… åˆ é™¤è®°å½•ä¿å­˜åˆ°: {removed_log_path}")

    # ========================================================================
    # æœ€ç»ˆç»Ÿè®¡
    # ========================================================================
    print("\n" + "=" * 70)
    print("ğŸ“Š æ¸…ç†å®Œæˆç»Ÿè®¡")
    print("=" * 70)

    print("\næ ·æœ¬æ•°å˜åŒ–:")
    print(f"   åŸå§‹: {original_count}")
    print(f"   æœ€ç»ˆ: {len(df)}")
    print(
        f"   åˆ é™¤: {original_count - len(df)} ({(original_count - len(df))/original_count*100:.2f}%)"
    )

    print("\nåˆ é™¤åŸå› åˆ†è§£:")
    for reason, ids in removed_samples.items():
        if len(ids) > 0:
            print(f"   - {reason}: {len(ids)} ä¸ªæ ·æœ¬")

    print("\næ ‡ç­¾åˆ†å¸ƒ:")
    label_dist = df["label"].value_counts()
    for label, count in label_dist.items():
        label_name = "phishing" if label == 1 else "benign"
        print(f"   - {label_name}: {count} ({count/len(df)*100:.1f}%)")

    print("\næ•°æ®è´¨é‡:")
    print(f"   - URLå”¯ä¸€æ€§: {df['url_text'].nunique()}/{len(df)}")
    print(
        f"   - HTMLå“ˆå¸Œå®Œæ•´æ€§: {df['html_sha1'].notna().sum()}/{len(df)} ({df['html_sha1'].notna().sum()/len(df)*100:.1f}%)"
    )
    print(
        f"   - IMGå“ˆå¸Œå®Œæ•´æ€§: {df['img_sha1'].notna().sum()}/{len(df)} ({df['img_sha1'].notna().sum()/len(df)*100:.1f}%)"
    )
    print(
        f"   - æ—¶é—´æˆ³æœ‰æ•ˆæ€§: {pd.to_datetime(df['timestamp'], errors='coerce').notna().sum()}/{len(df)} ({pd.to_datetime(df['timestamp'], errors='coerce').notna().sum()/len(df)*100:.1f}%)"
    )

    print("\n" + "=" * 70)
    print("âœ… æ•°æ®æ¸…ç†å®Œæˆï¼")
    print("=" * 70)
    print("\nğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
    print(f"   - æ¸…ç†åæ•°æ®: {output_path}")
    print(f"   - åŸå§‹å¤‡ä»½: {backup_path}")
    print(f"   - åˆ é™¤è®°å½•: {removed_log_path}")

    return 0


if __name__ == "__main__":
    exit(main())
