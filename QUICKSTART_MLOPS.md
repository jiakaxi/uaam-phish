# ğŸš€ å¿«é€Ÿå¼€å§‹æŒ‡å— - MLOps ç‰ˆæœ¬

> **æ›´æ–°æ—¥æœŸ:** 2025-10-22
> **ç‰ˆæœ¬:** 2.0 (MLOps Upgrade)

---

## ğŸ“¦ å®‰è£…

### 1. å…‹éš†é¡¹ç›®

```bash
git clone https://github.com/your-username/uaam-phish.git
cd uaam-phish
```

### 2. åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ

```bash
# åˆ›å»ºç¯å¢ƒ
python -m venv .venv

# æ¿€æ´»ç¯å¢ƒ
# Windows PowerShell:
.venv\Scripts\Activate.ps1
# Linux/Mac:
source .venv/bin/activate
```

### 3. å®‰è£…ä¾èµ–

```bash
# å®‰è£…é¡¹ç›®ï¼ˆå¯ç¼–è¾‘æ¨¡å¼ï¼‰
pip install -e .

# å®‰è£… MLOps å·¥å…·
pip install hydra-core wandb pre-commit

# å®‰è£… pre-commit hooks
pre-commit install
```

---

## ğŸ¯ å¿«é€Ÿè®­ç»ƒ

### æœ¬åœ°å¼€å‘ï¼ˆCPUï¼‰

```bash
# ä½¿ç”¨ Hydra é…ç½®
python scripts/train_hydra.py trainer=local

# æŸ¥çœ‹ç»“æœ
ls outputs/
```

### æœåŠ¡å™¨è®­ç»ƒï¼ˆGPUï¼‰

```bash
# ç™»å½• WandBï¼ˆé¦–æ¬¡ï¼‰
wandb login

# è®­ç»ƒå¹¶è·Ÿè¸ªå®éªŒ
export WANDB_PROJECT=uaam-phish
python scripts/train_hydra.py trainer=server logger=wandb
```

---

## ğŸ”§ å¸¸ç”¨å‘½ä»¤

### é…ç½®è¦†ç›–

```bash
# ä¿®æ”¹å­¦ä¹ ç‡
python scripts/train_hydra.py train.lr=2e-5

# ä¿®æ”¹æ‰¹æ¬¡å¤§å°å’Œdropout
python scripts/train_hydra.py train.bs=64 model.dropout=0.3

# ä½¿ç”¨ä¸åŒæ¨¡å‹
python scripts/train_hydra.py model.pretrained_name=bert-base-uncased
```

### è¶…å‚æ•°æœç´¢

```bash
# ç½‘æ ¼æœç´¢
python scripts/train_hydra.py -m \\
  train.lr=1e-5,2e-5,5e-5 \\
  model.dropout=0.1,0.2,0.3

# å…±9æ¬¡è¿è¡Œï¼ˆ3 lr Ã— 3 dropoutï¼‰
```

### å®éªŒç®¡ç†

```bash
# ä½¿ç”¨å®éªŒé…ç½®
python scripts/train_hydra.py experiment=url_baseline

# è‡ªå®šä¹‰å®éªŒåç§°
python scripts/train_hydra.py run.name=my_experiment logger=wandb
```

---

## ğŸ§ª æµ‹è¯•

```bash
# è¿è¡Œæ‰€æœ‰æµ‹è¯•
pytest tests/ -v

# è¿è¡Œç‰¹å®šæµ‹è¯•
pytest tests/test_data.py -v

# æŸ¥çœ‹è¦†ç›–ç‡
pytest tests/ --cov=src --cov-report=term
```

---

## ğŸ“Š æŸ¥çœ‹ç»“æœ

### WandB Dashboard

1. è®¿é—® https://wandb.ai
2. æŸ¥çœ‹é¡¹ç›®å®éªŒ
3. å¯¹æ¯”å¤šä¸ªè¿è¡Œ

### æœ¬åœ°ç»“æœ

```bash
# Hydra è¾“å‡ºç›®å½•
ls outputs/2025-10-22/18-45-00/

# ä¼ ç»Ÿå®éªŒç›®å½•
ls experiments/
```

---

## ğŸ’¡ å¸¸è§ä»»åŠ¡

### ä»»åŠ¡ 1: å¿«é€Ÿå®éªŒ

```bash
# å°æ•°æ®é›†å¿«é€ŸéªŒè¯
python scripts/train_hydra.py \\
  trainer=local \\
  data.sample_fraction=0.1 \\
  train.epochs=2
```

### ä»»åŠ¡ 2: æ­£å¼è®­ç»ƒ

```bash
# å®Œæ•´æ•°æ®é›†ï¼ŒGPUè®­ç»ƒ
python scripts/train_hydra.py \\
  trainer=server \\
  logger=wandb \\
  run.name=roberta_baseline_v1
```

### ä»»åŠ¡ 3: è¶…å‚æ•°è°ƒä¼˜

```bash
# æœç´¢æœ€ä½³å­¦ä¹ ç‡å’Œdropout
python scripts/train_hydra.py -m \\
  trainer=server \\
  logger=wandb \\
  train.lr=1e-5,2e-5,5e-5 \\
  model.dropout=0.1,0.2,0.3
```

---

## ğŸ“š æ–‡æ¡£ç´¢å¼•

- **æ€»ä½“æ¶æ„**: [README.md](README.md)
- **MLOpsæ”¹è¿›**: [MLOPS_IMPROVEMENTS_2025-10-22.md](docs/MLOPS_IMPROVEMENTS_2025-10-22.md)
- **WandBæŒ‡å—**: [WANDB_GUIDE.md](docs/WANDB_GUIDE.md)
- **CI/CDæŒ‡å—**: [CI_CD_GUIDE.md](docs/CI_CD_GUIDE.md)
- **é¡¹ç›®ç»“æ„**: [ROOT_STRUCTURE.md](docs/ROOT_STRUCTURE.md)

---

## âš ï¸ æ•…éšœæ’é™¤

### é—®é¢˜ 1: Hydra æ‰¾ä¸åˆ°é…ç½®

```bash
# ç¡®ä¿åœ¨é¡¹ç›®æ ¹ç›®å½•
cd /path/to/uaam-phish

# æ£€æŸ¥é…ç½®æ–‡ä»¶
ls configs/config.yaml
```

### é—®é¢˜ 2: WandB ç™»å½•å¤±è´¥

```bash
# é‡æ–°ç™»å½•
wandb login --relogin

# æˆ–ä½¿ç”¨ API key
export WANDB_API_KEY=64e15c91404e5023801580b0d943af3ebef4a033
```

### é—®é¢˜ 3: Pre-commit æ£€æŸ¥å¤±è´¥

```bash
# è‡ªåŠ¨ä¿®å¤
ruff check --fix .
black .

# é‡æ–°æäº¤
git add .
git commit -m "fix: ä¿®å¤ä»£ç æ ¼å¼"
```

---

## ğŸ“ ä¸‹ä¸€æ­¥

1. âœ… **é˜…è¯»æ–‡æ¡£**: æŸ¥çœ‹ [MLOPS_IMPROVEMENTS_2025-10-22.md](docs/MLOPS_IMPROVEMENTS_2025-10-22.md)
2. âœ… **è¿è¡Œç¤ºä¾‹**: å°è¯•ä¸Šè¿°å¿«é€Ÿè®­ç»ƒå‘½ä»¤
3. âœ… **æŸ¥çœ‹ WandB**: è®¿é—®ä½ çš„å®éªŒDashboard
4. âœ… **å­¦ä¹  Hydra**: äº†è§£é…ç½®è¦†ç›–å’Œç»„åˆ

---

**Happy Training! ğŸš€**
