# è®­ç»ƒæ“ä½œæ‰‹å†Œ

> **é€‚ç”¨åœºæ™¯**: å¤§æ•°æ®é›†è®­ç»ƒ + å¤šæ¨¡å‹å®éªŒ
> **æ›´æ–°æ—¥æœŸ**: 2025-10-23

---

## ğŸ¯ è®­ç»ƒåœºæ™¯é€ŸæŸ¥è¡¨

| åœºæ™¯ | å‘½ä»¤ | é¢„ä¼°æ—¶é—´ |
|------|------|----------|
| **å¿«é€ŸéªŒè¯** | `python scripts/train_hydra.py trainer=local data.sample_fraction=0.1` | 5-10åˆ†é’Ÿ |
| **å°æ•°æ®é›†å®Œæ•´è®­ç»ƒ** | `python scripts/train_hydra.py trainer=server logger=wandb` | 10-20åˆ†é’Ÿ |
| **å¤§æ•°æ®é›†è®­ç»ƒ** | `python scripts/train_hydra.py experiment=url_large_baseline` | 1-3å°æ—¶ |
| **å¤šGPUè®­ç»ƒ** | `python scripts/train_hydra.py trainer=multi_gpu logger=wandb` | 30åˆ†-1å°æ—¶ |
| **è¶…å‚æ•°æœç´¢** | `python scripts/train_hydra.py -m train.lr=1e-3,5e-4,1e-4` | æ•°å°æ—¶ |

---

## ğŸ“‹ è®­ç»ƒå‰æ£€æŸ¥æ¸…å•

### âœ… ç¯å¢ƒæ£€æŸ¥

```bash
# 1. æ£€æŸ¥Pythonç¯å¢ƒ
python --version  # åº”è¯¥æ˜¯ 3.8+

# 2. æ£€æŸ¥PyTorch
python -c "import torch; print(torch.__version__); print(torch.cuda.is_available())"

# 3. æ£€æŸ¥CUDA
nvidia-smi  # æŸ¥çœ‹GPUçŠ¶æ€

# 4. æ£€æŸ¥ä¾èµ–
pip list | grep -E "torch|lightning|hydra|wandb"
```

### âœ… æ•°æ®æ£€æŸ¥

```bash
# 1. éªŒè¯æ•°æ®schema
python scripts/validate_data_schema.py

# 2. æŸ¥çœ‹æ•°æ®ç»Ÿè®¡
python -c "
import pandas as pd
import os
data_root = os.environ.get('DATA_ROOT', 'data/processed')
for split in ['train', 'val', 'test']:
    path = f'{data_root}/url_{split}.csv'
    if os.path.exists(path):
        df = pd.read_csv(path)
        pos = (df['label'] == 1).sum()
        neg = (df['label'] == 0).sum()
        print(f'{split}: {len(df)} samples (Phish: {pos}, Legit: {neg})')
"

# 3. æ£€æŸ¥æ•°æ®é‡å 
python check_overlap.py
```

### âœ… é…ç½®æ£€æŸ¥

```bash
# æŸ¥çœ‹Hydraé…ç½®
python scripts/train_hydra.py --help

# é¢„è§ˆé…ç½®ï¼ˆä¸è¿è¡Œï¼‰
python scripts/train_hydra.py --cfg job

# æ£€æŸ¥ç‰¹å®šé…ç½®
python scripts/train_hydra.py experiment=url_large_baseline --cfg job
```

---

## ğŸš€ æ ‡å‡†è®­ç»ƒæµç¨‹

### åœºæ™¯ 1: å°æ•°æ®é›†ï¼ˆå½“å‰ï¼‰

```bash
# Step 1: å¿«é€ŸéªŒè¯é…ç½®
python scripts/train_hydra.py \
  trainer=local \
  data.sample_fraction=0.1 \
  train.epochs=2

# Step 2: å®Œæ•´è®­ç»ƒ
python scripts/train_hydra.py \
  trainer=server \
  logger=wandb \
  run.name=url_small_baseline

# Step 3: æŸ¥çœ‹ç»“æœ
python scripts/compare_experiments.py --latest 1
```

### åœºæ™¯ 2: å¤§æ•°æ®é›†

#### å‡†å¤‡é˜¶æ®µ

```bash
# 1. è®¾ç½®æ•°æ®è·¯å¾„
# Windows PowerShell:
$env:DATA_ROOT = "D:\large_phish_dataset\processed"
$env:WANDB_PROJECT = "uaam-phish-large"

# Linux/Mac:
export DATA_ROOT=/data/large_phish_dataset/processed
export WANDB_PROJECT=uaam-phish-large

# 2. éªŒè¯æ•°æ®
python scripts/validate_data_schema.py

# 3. å¿«é€Ÿæµ‹è¯•ï¼ˆ10%æ•°æ®ï¼‰
python scripts/train_hydra.py \
  trainer=server \
  data.num_workers=16 \
  data.sample_fraction=0.1 \
  train.epochs=5 \
  run.name=large_quick_test
```

#### æ­£å¼è®­ç»ƒ

```bash
# ä½¿ç”¨å®éªŒé…ç½®ï¼ˆæ¨èï¼‰
python scripts/train_hydra.py \
  experiment=url_large_baseline

# æˆ–æ‰‹åŠ¨æŒ‡å®š
python scripts/train_hydra.py \
  data=url_large \
  trainer=server \
  logger=wandb \
  run.name=url_large_v1
```

#### è¶…å‚æ•°è°ƒä¼˜

```bash
# æœç´¢æœ€ä½³å­¦ä¹ ç‡
python scripts/train_hydra.py -m \
  experiment=url_large_baseline \
  train.lr=1e-3,5e-4,1e-4,5e-5 \
  run.name=lr_search

# æœç´¢dropout
python scripts/train_hydra.py -m \
  experiment=url_large_baseline \
  model.dropout=0.1,0.2,0.3 \
  run.name=dropout_search
```

### åœºæ™¯ 3: å¤šGPUè®­ç»ƒ

```bash
# æ–¹å¼1: ä½¿ç”¨multi_gpué…ç½®
python scripts/train_hydra.py \
  trainer=multi_gpu \
  data=url_large \
  logger=wandb \
  run.name=url_large_multigpu

# æ–¹å¼2: å‘½ä»¤è¡Œè¦†ç›–
python scripts/train_hydra.py \
  experiment=url_large_baseline \
  trainer.hardware.devices=-1 \
  trainer.hardware.strategy=ddp \
  trainer.metrics.dist.sync_metrics=true
```

### åœºæ™¯ 4: åè®®å¯¹æ¯”

```bash
# è¿è¡Œæ‰€æœ‰åè®®
.\scripts\run_all_protocols.ps1

# æˆ–æ‰‹åŠ¨è¿è¡Œ
python scripts/train_hydra.py protocol=random run.name=large_random
python scripts/train_hydra.py protocol=temporal run.name=large_temporal
python scripts/train_hydra.py protocol=brand_ood run.name=large_brand_ood

# å¯¹æ¯”ç»“æœ
python scripts/compare_experiments.py \
  --exp_names large_random large_temporal large_brand_ood \
  --metric auroc
```

---

## ğŸ”§ å¸¸è§è®­ç»ƒé—®é¢˜

### é—®é¢˜ 1: CUDA Out of Memory

**ç—‡çŠ¶**:
```
RuntimeError: CUDA out of memory. Tried to allocate X.XX GiB
```

**è§£å†³æ–¹æ¡ˆ**:

```bash
# æ–¹æ¡ˆA: å‡å°æ‰¹æ¬¡å¤§å°
python scripts/train_hydra.py \
  train.batch_size=32  # ä»128é™åˆ°32

# æ–¹æ¡ˆB: ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯
python scripts/train_hydra.py \
  train.batch_size=16 \
  train.accumulate_grad_batches=4  # ç­‰æ•ˆbatch_size=64

# æ–¹æ¡ˆC: é™ä½ç²¾åº¦
python scripts/train_hydra.py \
  trainer.hardware.precision=16-mixed
```

### é—®é¢˜ 2: è®­ç»ƒå¤ªæ…¢

**è¯Šæ–­**:

```bash
# æ£€æŸ¥æ˜¯å¦ä½¿ç”¨GPU
python -c "import torch; print(torch.cuda.is_available())"

# æ£€æŸ¥æ•°æ®åŠ è½½
# åœ¨train_hydra.pyä¸­æ·»åŠ :
# profiler = SimpleProfiler()  # PyTorch Lightning
```

**ä¼˜åŒ–**:

```bash
# å¢åŠ num_workers
python scripts/train_hydra.py data.num_workers=16

# ä½¿ç”¨æ›´å¤§æ‰¹æ¬¡
python scripts/train_hydra.py train.batch_size=128

# ä½¿ç”¨æ··åˆç²¾åº¦
python scripts/train_hydra.py trainer.hardware.precision=16-mixed

# ä½¿ç”¨å¤šGPU
python scripts/train_hydra.py trainer=multi_gpu
```

### é—®é¢˜ 3: è¿‡æ‹Ÿåˆ

**ç—‡çŠ¶**: è®­ç»ƒé›†å‡†ç¡®ç‡é«˜ï¼ŒéªŒè¯é›†å‡†ç¡®ç‡ä½

**è§£å†³**:

```bash
# å¢åŠ dropout
python scripts/train_hydra.py model.dropout=0.3

# ä½¿ç”¨æ•°æ®å¢å¼ºï¼ˆå¦‚æœå®ç°ï¼‰
python scripts/train_hydra.py data.augmentation.enabled=true

# å‡å°‘epochs
python scripts/train_hydra.py train.epochs=20

# æ—©åœ
python scripts/train_hydra.py train.patience=5
```

### é—®é¢˜ 4: æ¬ æ‹Ÿåˆ

**ç—‡çŠ¶**: è®­ç»ƒé›†å’ŒéªŒè¯é›†å‡†ç¡®ç‡éƒ½ä½

**è§£å†³**:

```bash
# å¢åŠ æ¨¡å‹å®¹é‡
python scripts/train_hydra.py model.hidden_dim=256

# å¢åŠ è®­ç»ƒæ—¶é—´
python scripts/train_hydra.py train.epochs=100

# è°ƒæ•´å­¦ä¹ ç‡
python scripts/train_hydra.py train.lr=1e-3

# å‡å°dropout
python scripts/train_hydra.py model.dropout=0.05
```

---

## ğŸ“Š ç›‘æ§è®­ç»ƒ

### WandB Dashboard

ç™»å½• https://wandb.ai æŸ¥çœ‹ï¼š

1. **å®æ—¶æŒ‡æ ‡**
   - Lossæ›²çº¿
   - Accuracy/F1/AUROC
   - å­¦ä¹ ç‡å˜åŒ–

2. **ç³»ç»Ÿç›‘æ§**
   - GPUåˆ©ç”¨ç‡
   - å†…å­˜ä½¿ç”¨
   - CPUä½¿ç”¨ç‡

3. **è¶…å‚æ•°å¯¹æ¯”**
   - å¹¶è¡Œå®éªŒå¯¹æ¯”
   - å‚æ•°é‡è¦æ€§åˆ†æ

### æœ¬åœ°ç›‘æ§

```bash
# æŸ¥çœ‹æ—¥å¿—
tail -f experiments/<run_name>/logs/train.log

# æŸ¥çœ‹GPUä½¿ç”¨
watch -n 1 nvidia-smi

# æŸ¥çœ‹ç³»ç»Ÿèµ„æº
htop  # Linux
```

---

## ğŸ¯ è®­ç»ƒå®Œæˆå

### 1. æŸ¥çœ‹ç»“æœ

```bash
# æŸ¥çœ‹æœ€æ–°å®éªŒ
cd experiments/
ls -lt | head -5

# æŸ¥çœ‹æŒ‡æ ‡
cat <run_name>/results/metrics_*.json

# æŸ¥çœ‹å¯è§†åŒ–
# experiments/<run_name>/results/*.png
```

### 2. å¯¹æ¯”å®éªŒ

```bash
# å¯¹æ¯”æœ€è¿‘5ä¸ª
python scripts/compare_experiments.py --latest 5

# æŸ¥æ‰¾æœ€ä½³
python scripts/compare_experiments.py \
  --find_best \
  --metric auroc

# å¯¼å‡ºç»“æœ
python scripts/compare_experiments.py \
  --latest 10 \
  --output comparison.csv
```

### 3. åŠ è½½æœ€ä½³æ¨¡å‹

```python
import pytorch_lightning as pl
from src.systems.url_only_module import UrlOnlySystem

# åŠ è½½æ£€æŸ¥ç‚¹
checkpoint_path = "experiments/<run_name>/checkpoints/best-*.ckpt"
model = UrlOnlySystem.load_from_checkpoint(checkpoint_path)

# æ¨ç†
model.eval()
predictions = model(batch)
```

---

## ğŸ“ å®éªŒè®°å½•æ¨¡æ¿

### WandBç¬”è®°æ¨¡æ¿

```markdown
## å®éªŒ: <å®éªŒåç§°>

### ç›®æ ‡
- éªŒè¯XXXå‡è®¾
- å¯¹æ¯”XXXé…ç½®

### é…ç½®
- æ¨¡å‹: URLEncoder
- æ•°æ®: å¤§æ•°æ®é›† (N samples)
- å­¦ä¹ ç‡: 5e-4
- Batch size: 128

### ç»“æœ
- AUROC: 0.XX
- F1: 0.XX
- ECE: 0.XX

### ç»“è®º
- XXXæ•ˆæœæ›´å¥½
- ä¸‹ä¸€æ­¥: XXX

### é—®é¢˜
- é‡åˆ°XXXé—®é¢˜
- è§£å†³æ–¹æ¡ˆ: XXX
```

---

## ğŸ”„ è®­ç»ƒè¿­ä»£æµç¨‹

```
1. å¿«é€ŸéªŒè¯
   â†“
2. å°è§„æ¨¡è®­ç»ƒ
   â†“
3. åˆ†æç»“æœ â†’ è°ƒæ•´é…ç½®
   â†“
4. å¤§è§„æ¨¡è®­ç»ƒ
   â†“
5. è¶…å‚æ•°æœç´¢
   â†“
6. æœ€ç»ˆæ¨¡å‹
```

---

## ğŸ“ æœ€ä½³å®è·µ

### 1. æ¸è¿›å¼è®­ç»ƒ

```bash
# 1. å¿«é€Ÿæµ‹è¯•ï¼ˆ10%æ•°æ®ï¼Œ2 epochsï¼‰
python scripts/train_hydra.py \
  trainer=local \
  data.sample_fraction=0.1 \
  train.epochs=2

# 2. ä¸­ç­‰è§„æ¨¡ï¼ˆ30%æ•°æ®ï¼Œ10 epochsï¼‰
python scripts/train_hydra.py \
  trainer=server \
  data.sample_fraction=0.3 \
  train.epochs=10

# 3. å®Œæ•´è®­ç»ƒï¼ˆ100%æ•°æ®ï¼Œ50 epochsï¼‰
python scripts/train_hydra.py \
  experiment=url_large_baseline
```

### 2. ä½¿ç”¨æœ‰æ„ä¹‰çš„å®éªŒå

```bash
# âŒ ä¸å¥½
python scripts/train_hydra.py run.name=exp1

# âœ… å¥½
python scripts/train_hydra.py \
  run.name=url_large_lr5e4_bs128_dropout02
```

### 3. è®°å½•å®éªŒ

- åœ¨WandBä¸­æ·»åŠ noteså’Œtags
- ä¿å­˜å…³é”®å†³ç­–åˆ°æ–‡æ¡£
- å®šæœŸå¯¹æ¯”å®éªŒç»“æœ

### 4. ç‰ˆæœ¬æ§åˆ¶

```bash
# è®­ç»ƒå‰æäº¤ä»£ç 
git add .
git commit -m "config: å‡†å¤‡å¤§æ•°æ®é›†è®­ç»ƒ"

# è®°å½•commit hash
git rev-parse HEAD > experiments/<run_name>/git_commit.txt
```

---

## ğŸš¨ ç´§æ€¥æƒ…å†µå¤„ç†

### è®­ç»ƒä¸­æ–­

```bash
# Lightningè‡ªåŠ¨ä¿å­˜æ£€æŸ¥ç‚¹
# æŸ¥æ‰¾æœ€æ–°æ£€æŸ¥ç‚¹
ls experiments/<run_name>/checkpoints/

# ä»æ£€æŸ¥ç‚¹æ¢å¤ï¼ˆéœ€è¦åœ¨ä»£ç ä¸­å®ç°resumeé€»è¾‘ï¼‰
python scripts/train_hydra.py \
  run.name=<run_name>_resume \
  resume_from_checkpoint=experiments/<run_name>/checkpoints/last.ckpt
```

### æ¸…ç†ç£ç›˜ç©ºé—´

```bash
# åˆ é™¤æ—§å®éªŒï¼ˆä¿ç•™æœ€è¿‘30ä¸ªï¼‰
cd experiments/
ls -t | tail -n +31 | xargs rm -rf

# åˆ é™¤ä¸­é—´æ£€æŸ¥ç‚¹ï¼ˆåªä¿ç•™bestï¼‰
find . -name "epoch=*.ckpt" -not -name "best*" -delete
```

---

---

## ğŸ“Š è®­ç»ƒç»“æœå¯¹æ¯”åˆ†æ

### âœ… æˆåŠŸé…ç½®ï¼ˆå‡†ç¡®ç‡ 99.01%ï¼‰
```yaml
æ¨¡å‹é…ç½®:
  dropout: 0.1          # è¾ƒå°çš„dropout

è®­ç»ƒé…ç½®:
  epochs: 50            # 50è½®è®­ç»ƒ
  batch_size: 64        # è¾ƒå¤§çš„batch size
  lr: 0.0001            # å­¦ä¹ ç‡ 1e-4
  patience: 5
```

**è®­ç»ƒç»“æœ**ï¼š
- æµ‹è¯•é›†å‡†ç¡®ç‡: 99.01% (100/101)
- F1åˆ†æ•°: 98.08%
- AUROC: 63.37%
- æ¨¡å‹æ”¶æ•›è‰¯å¥½

### âŒ å¤±è´¥é…ç½®ï¼ˆå‡†ç¡®ç‡ 53.47%ï¼‰
```yaml
æ¨¡å‹é…ç½®:
  dropout: 0.2          # æ›´å¤§çš„dropout (2å€)

è®­ç»ƒé…ç½®:
  epochs: 10            # ä»…10è½®è®­ç»ƒ (å‡å°‘5å€)
  batch_size: 32        # è¾ƒå°çš„batch size (å‡å°‘ä¸€åŠ)
  lr: 2e-05             # å­¦ä¹ ç‡ 0.00002 (å‡å°‘5å€!)

ç¡¬ä»¶é…ç½®:
  accelerator: cpu      # ä½¿ç”¨CPU
```

**è®­ç»ƒç»“æœ**ï¼š
- éªŒè¯é›†å‡†ç¡®ç‡: 53.47%
- æ¨¡å‹æœªæ”¶æ•›ï¼Œå§‹ç»ˆé¢„æµ‹ç±»åˆ«1

---

## ğŸ”´ å…³é”®é—®é¢˜åˆ†æ

### é—®é¢˜1: å­¦ä¹ ç‡è¿‡ä½
- **æˆåŠŸ**: lr=1e-4 (0.0001)
- **å¤±è´¥**: lr=2e-5 (0.00002) - **å‡å°‘5å€**
- **å½±å“**: å­¦ä¹ ç‡å¤ªä½å¯¼è‡´æ¨¡å‹æ— æ³•æœ‰æ•ˆå­¦ä¹ 

### é—®é¢˜2: è®­ç»ƒè½®æ•°ä¸è¶³
- **æˆåŠŸ**: epochs=50
- **å¤±è´¥**: epochs=10 - **å‡å°‘5å€**
- **å½±å“**: æ¨¡å‹æ²¡æœ‰è¶³å¤Ÿæ—¶é—´æ”¶æ•›

### é—®é¢˜3: Batch Sizeè¿‡å°
- **æˆåŠŸ**: batch_size=64
- **å¤±è´¥**: batch_size=32 - **å‡å°‘ä¸€åŠ**
- **å½±å“**: æ¢¯åº¦ä¼°è®¡ä¸ç¨³å®š

### é—®é¢˜4: Dropoutè¿‡å¤§
- **æˆåŠŸ**: dropout=0.1
- **å¤±è´¥**: dropout=0.2 - **å¢åŠ ä¸€å€**
- **å½±å“**: è¿‡åº¦æ­£åˆ™åŒ–ï¼ŒæŠ‘åˆ¶å­¦ä¹ 

---

## ğŸ“ˆ è®­ç»ƒè¿‡ç¨‹ç›‘æ§

### é˜¶æ®µ1: å¿«é€Ÿå­¦ä¹  (Epochs 1-10)
- æŸå¤±å¿«é€Ÿä¸‹é™
- å‡†ç¡®ç‡ä» ~50% â†’ ~90%
- æ¨¡å‹å¼€å§‹å­¦ä¹ URLç‰¹å¾

### é˜¶æ®µ2: ç²¾ç»†è°ƒä¼˜ (Epochs 10-30)
- å‡†ç¡®ç‡ç¨³æ­¥æå‡åˆ° 95%+
- AUROC æå‡åˆ° 0.95+
- æ¨¡å‹åŒºåˆ†èƒ½åŠ›å¢å¼º

### é˜¶æ®µ3: æ”¶æ•› (Epochs 30-50)
- æŒ‡æ ‡è¶‹äºç¨³å®š
- éªŒè¯é›†æ€§èƒ½æœ€ä¼˜
- Early stopping å¯èƒ½ä¼šæå‰ç»ˆæ­¢

---

## ğŸ¯ é¢„æœŸç»“æœ

- **å‡†ç¡®ç‡**: 95-99%ï¼ˆä¹‹å‰: 53%ï¼‰
- **AUROC**: > 0.95ï¼ˆä¹‹å‰: 0.10ï¼‰
- **è®­ç»ƒæ—¶é—´**: 10-15åˆ†é’Ÿ
- **æ”¶æ•›è½®æ•°**: çº¦30è½®

è®­ç»ƒæˆåŠŸï¼ ğŸ‰

---

**å‡†å¤‡å¥½å¼€å§‹å¤§è§„æ¨¡è®­ç»ƒäº†ï¼ç¥è®­ç»ƒé¡ºåˆ©ï¼** ğŸš€
