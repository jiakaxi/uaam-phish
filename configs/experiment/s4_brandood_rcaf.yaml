# @package _global_

# S4 Brand-OOD: Adaptive Fusion with Learned Lambda_c (RCAF Full)

defaults:
  - override /model: multimodal_baseline
  - override /trainer: default
  - override /logger: wandb

run:
  name: s4_brandood_rcaf
  seed: 42
  tags: ["s4", "brandood", "adaptive", "rcaf"]
  notes: "S4 Brand-OOD: Adaptive fusion with learned Î»_c for OOD generalization"

protocol: presplit

# ===== System Configuration =====
system:
  _target_: src.systems.s4_rcaf_system.S4RCAFSystem
  _recursive_: false
  # Encoder params
  url_vocab_size: ${model.url_vocab_size}
  url_embed_dim: ${model.url_embed_dim}
  url_hidden_dim: ${model.url_hidden_dim}
  url_num_layers: ${model.url_num_layers}
  url_max_len: ${model.url_max_len}
  html_model_name: ${model.html_model_name}
  html_hidden_dim: ${model.html_hidden_dim}
  html_projection_dim: ${model.html_projection_dim}
  html_freeze_bert: ${model.html_freeze_bert}
  visual_model_name: ${model.visual_model_name}
  visual_pretrained: ${model.visual_pretrained}
  visual_projection_dim: ${model.visual_projection_dim}
  visual_freeze_backbone: ${model.visual_freeze_backbone}
  # Training params
  learning_rate: 1.0e-4
  weight_decay: 1.0e-4
  dropout: 0.3
  pos_weight: null

# ===== Fusion Configuration =====
fusion:
  hidden_dim: 16
  temperature: 2.0
  warmup_epochs: 5
  lambda_regularization: 0.01

# ===== Optimizer Configuration =====
optimizer:
  encoder_lr: 1.0e-4
  fusion_lr: 1.0e-3

# ===== Scheduler Configuration =====
scheduler:
  type: cosine
  warmup_steps: 500

# ===== Module Configuration =====
# REQUIRED: Both U-Module and C-Module must be enabled for S4
modules:
  use_umodule: true
  use_cmodule: true

# ===== U-Module Configuration =====
umodule:
  enabled: true
  mc_iters: 10
  dropout: 0.3
  lambda_u: 1.0
  temperature_init: 1.0
  learnable: false

# ===== C-Module Configuration =====
c_module:
  model_name: sentence-transformers/all-MiniLM-L6-v2
  thresh: 0.60
  brand_lexicon_path: ${paths.assets}/brand_lexicon.txt
  use_ocr: true

# ===== Metrics Configuration =====
metrics:
  consistency_thresh: 0.60

# ===== Training Configuration =====
train:
  epochs: 50
  lr: 1.0e-4
  bs: 32
  weight_decay: 1.0e-4
  grad_accumulation: 1
  log_every: 25

# ===== Evaluation Configuration =====
eval:
  monitor: val/auroc
  patience: 10
  mode: max

# ===== Data Configuration =====
datamodule:
  _target_: src.data.multimodal_datamodule.MultimodalDataModule
  _recursive_: false
  train_csv: workspace/data/splits/brandood/train_cached.csv
  val_csv: workspace/data/splits/brandood/val_cached.csv
  test_csv: workspace/data/splits/brandood/test_id_cached.csv  # Use test_id (in-distribution test)
  image_dir: data/processed/screenshots
  corrupt_root: workspace/data/corrupt
  batch_size: 32
  num_workers: 4
  pin_memory: true
  persistent_workers: true
  prefetch_factor: 2
  use_augmentation: false
  url_max_len: 200
  url_vocab_size: 128
  html_max_len: 256
  preload_html: false
  preprocessed_train_dir: workspace/data/preprocessed/brandood/train
  preprocessed_val_dir: workspace/data/preprocessed/brandood/val
  preprocessed_test_dir: workspace/data/preprocessed/brandood/test

# ===== Paths Configuration =====
paths:
  output_dir: workspace/runs/${run.name}/seed_${run.seed}
  assets: resources

# ===== Hardware Configuration =====
hardware:
  accelerator: gpu
  devices: 1
  precision: 16-mixed

# ===== Trainer Configuration =====
trainer:
  max_epochs: ${train.epochs}
  accelerator: ${hardware.accelerator}
  devices: ${hardware.devices}
  precision: ${hardware.precision}
  gradient_clip_val: 1.0
  accumulate_grad_batches: ${train.grad_accumulation}
  log_every_n_steps: ${train.log_every}
  check_val_every_n_epoch: 1
  enable_checkpointing: true
  enable_progress_bar: true
  enable_model_summary: true
  deterministic: false  # Set to true for reproducibility (slower)
