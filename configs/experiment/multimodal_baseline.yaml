# @package _global_
#
# 多模态拼接基线实验配置（S0，参见论文 Sec. 4.6）
#
# 运行示例：
#   python scripts/train_hydra.py experiment=multimodal_baseline
#   python scripts/train_hydra.py experiment=multimodal_baseline datamodule.split_protocol=random
#   python scripts/train_hydra.py experiment=multimodal_baseline datamodule.split_protocol=temporal

defaults:
  - override /model: multimodal_baseline
  - override /trainer: default

# 明确指定 system 与 datamodule（_recursive_=false 以避免重复实例化）
system:
  _target_: src.systems.multimodal_baseline.MultimodalBaselineSystem
  _recursive_: false
  # 继承模型配置参数
  url_vocab_size: ${model.url_vocab_size}
  url_embed_dim: ${model.url_embed_dim}
  url_hidden_dim: ${model.url_hidden_dim}
  url_num_layers: ${model.url_num_layers}
  url_max_len: ${model.url_max_len}
  html_model_name: ${model.html_model_name}
  html_hidden_dim: ${model.html_hidden_dim}
  html_projection_dim: ${model.html_projection_dim}
  html_freeze_bert: ${model.html_freeze_bert}
  visual_model_name: ${model.visual_model_name}
  visual_pretrained: ${model.visual_pretrained}
  visual_projection_dim: ${model.visual_projection_dim}
  visual_freeze_backbone: ${model.visual_freeze_backbone}
  learning_rate: ${model.learning_rate}
  weight_decay: ${model.weight_decay}
  dropout: ${model.dropout}
  pos_weight: ${model.pos_weight}

datamodule:
  _target_: src.data.multimodal_datamodule.MultimodalDataModule
  _recursive_: false
  master_csv: "data/processed/master_v2.csv"
  split_protocol: "presplit"  # Sec. 4.3.4 默认遵循提供的 split 列
  use_presplit: true
  batch_size: 128  # Sec. 4.6.3 目标 batch size（如显存不足可调 accumulate_grad_batches）
  num_workers: 0  # Windows优化：单进程避免多进程开销
  pin_memory: true
  use_augmentation: false
  url_max_len: 200
  url_vocab_size: 128
  html_max_len: 256  # Sec. 4.6.1 默认截断；可通过覆盖提升至 512
  image_dir: "data/processed/screenshots"

# 路径配置
paths:
  data_dir: "data"
  output_dir: "${hydra:runtime.output_dir}"

# 运行元数据
run:
  name: "mm_baseline_${datamodule.split_protocol}_${now:%Y%m%d_%H%M%S}"
  tags: ["multimodal", "baseline", "concat", "early_fusion", "${datamodule.split_protocol}"]
  notes: "S0: Multimodal concatenation baseline (Early Fusion, logits-only head)"

# Trainer 配置（其余默认由 configs/trainer/default.yaml 提供）
trainer:
  max_epochs: 25
  check_val_every_n_epoch: 1
  log_every_n_steps: 50
  precision: 16
  gradient_clip_val: 1.0
  accumulate_grad_batches: 1  # 若需降 batch size，请相应调大该值

  callbacks:
    - _target_: pytorch_lightning.callbacks.ModelCheckpoint
      dirpath: "${paths.output_dir}/checkpoints"
      filename: "epoch={epoch:02d}-val_auroc={val/auroc:.3f}"
      monitor: "val/auroc"
      mode: "max"
      save_top_k: 3
      save_last: true
      auto_insert_metric_name: false

    - _target_: pytorch_lightning.callbacks.EarlyStopping
      monitor: "val/auroc"
      patience: 10
      mode: "max"
      min_delta: 0.0

    - _target_: pytorch_lightning.callbacks.LearningRateMonitor
      logging_interval: "epoch"

# Logger 设置
logger:
  wandb:
    project: "uaam-phish"
    name: ${run.name}
    tags: ${run.tags}
    notes: ${run.notes}
    save_dir: "${paths.output_dir}"

# Seed for reproducibility
seed: 42
